{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bismillah Bisa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Nama Fitur</th>\n",
       "      <th>User Story</th>\n",
       "      <th>Scenario</th>\n",
       "      <th>Scenario.1</th>\n",
       "      <th>Scenario.2</th>\n",
       "      <th>Scenario.3</th>\n",
       "      <th>Scenario.4</th>\n",
       "      <th>Scenario.5</th>\n",
       "      <th>Scenario.6</th>\n",
       "      <th>Scenario.7</th>\n",
       "      <th>Scenario.8</th>\n",
       "      <th>Scenario.9</th>\n",
       "      <th>Scenario.10</th>\n",
       "      <th>Effort</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Create Program Outcomes</td>\n",
       "      <td>In order to establish Program Outcomes\\nAs Cur...</td>\n",
       "      <td>Scenario: Create Program Outcomes Successfully...</td>\n",
       "      <td>Scenario: Failed to create Program Outcomes as...</td>\n",
       "      <td>Scenario: Failed to Create Program Outcomes Du...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Read Program Outcomes</td>\n",
       "      <td>In order to evaluate Program Outcomes\\nAs Curr...</td>\n",
       "      <td>Scenario: Program Outcomes Provided\\nGiven I a...</td>\n",
       "      <td>Scenario: Empty Program Outcomes\\nGiven I am o...</td>\n",
       "      <td>Scenario: Program Outcomes Export to Excel\\nGi...</td>\n",
       "      <td>Scenario: Program Outcomes Export to PDF\\nGive...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Update Program Outcomes</td>\n",
       "      <td>In order to improve Program Outcomes\\nAs Curri...</td>\n",
       "      <td>Scenario: Update Program Outcomes Successfully...</td>\n",
       "      <td>Scenario: Failed to update Program Outcomes as...</td>\n",
       "      <td>Scenario: Failed to update Program Outcomes Du...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Delete Program Outcomes</td>\n",
       "      <td>In order to remove irrelevant Program Outcomes...</td>\n",
       "      <td>Scenario: Delete Program Outcomes Successfully...</td>\n",
       "      <td>Scenario: Failed to delete Program Outcomes Du...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Create SNDikti Learning Outcomes</td>\n",
       "      <td>In order to adapt SNDikti Learning Outcomes\\nA...</td>\n",
       "      <td>Scenario: Create SNDikti Learning Outcome Succ...</td>\n",
       "      <td>Scenario: Failed to create SNDikti Learning Ou...</td>\n",
       "      <td>Scenario: Failed to Create SNDikti Learning Ou...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>Read Study Program</td>\n",
       "      <td>In order to verify Study Program Data\\nAs Curr...</td>\n",
       "      <td>Scenario: Study Program Provided\\nGiven I am o...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>Update Study Program</td>\n",
       "      <td>In order to improve Study Program Data \\nAs Cu...</td>\n",
       "      <td>Scenario: Update Study Program Successfully\\nG...</td>\n",
       "      <td>Scenario: Failed to update Study Program Due t...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>Read User Management</td>\n",
       "      <td>In order to verify User\\nAs Curriculum Team,\\n...</td>\n",
       "      <td>Scenario: User Provided\\nGiven I am on “http:/...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>Update User Management</td>\n",
       "      <td>In order to improve User Data \\nAs Curriculum ...</td>\n",
       "      <td>Scenario: Update User Successfully\\nGiven I am...</td>\n",
       "      <td>Scenario: Failed to update User Due to Empty D...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>Delete User Management</td>\n",
       "      <td>In order to remove irrelevant User\\nAs Curricu...</td>\n",
       "      <td>Scenario: Delete User Management Successfully\\...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>73 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Nama Fitur  \\\n",
       "0            Create Program Outcomes   \n",
       "1              Read Program Outcomes   \n",
       "2            Update Program Outcomes   \n",
       "3            Delete Program Outcomes   \n",
       "4   Create SNDikti Learning Outcomes   \n",
       "..                               ...   \n",
       "68                Read Study Program   \n",
       "69              Update Study Program   \n",
       "70              Read User Management   \n",
       "71            Update User Management   \n",
       "72            Delete User Management   \n",
       "\n",
       "                                           User Story  \\\n",
       "0   In order to establish Program Outcomes\\nAs Cur...   \n",
       "1   In order to evaluate Program Outcomes\\nAs Curr...   \n",
       "2   In order to improve Program Outcomes\\nAs Curri...   \n",
       "3   In order to remove irrelevant Program Outcomes...   \n",
       "4   In order to adapt SNDikti Learning Outcomes\\nA...   \n",
       "..                                                ...   \n",
       "68  In order to verify Study Program Data\\nAs Curr...   \n",
       "69  In order to improve Study Program Data \\nAs Cu...   \n",
       "70  In order to verify User\\nAs Curriculum Team,\\n...   \n",
       "71  In order to improve User Data \\nAs Curriculum ...   \n",
       "72  In order to remove irrelevant User\\nAs Curricu...   \n",
       "\n",
       "                                             Scenario  \\\n",
       "0   Scenario: Create Program Outcomes Successfully...   \n",
       "1   Scenario: Program Outcomes Provided\\nGiven I a...   \n",
       "2   Scenario: Update Program Outcomes Successfully...   \n",
       "3   Scenario: Delete Program Outcomes Successfully...   \n",
       "4   Scenario: Create SNDikti Learning Outcome Succ...   \n",
       "..                                                ...   \n",
       "68  Scenario: Study Program Provided\\nGiven I am o...   \n",
       "69  Scenario: Update Study Program Successfully\\nG...   \n",
       "70  Scenario: User Provided\\nGiven I am on “http:/...   \n",
       "71  Scenario: Update User Successfully\\nGiven I am...   \n",
       "72  Scenario: Delete User Management Successfully\\...   \n",
       "\n",
       "                                           Scenario.1  \\\n",
       "0   Scenario: Failed to create Program Outcomes as...   \n",
       "1   Scenario: Empty Program Outcomes\\nGiven I am o...   \n",
       "2   Scenario: Failed to update Program Outcomes as...   \n",
       "3   Scenario: Failed to delete Program Outcomes Du...   \n",
       "4   Scenario: Failed to create SNDikti Learning Ou...   \n",
       "..                                                ...   \n",
       "68                                                      \n",
       "69  Scenario: Failed to update Study Program Due t...   \n",
       "70                                                      \n",
       "71  Scenario: Failed to update User Due to Empty D...   \n",
       "72                                                      \n",
       "\n",
       "                                           Scenario.2  \\\n",
       "0   Scenario: Failed to Create Program Outcomes Du...   \n",
       "1   Scenario: Program Outcomes Export to Excel\\nGi...   \n",
       "2   Scenario: Failed to update Program Outcomes Du...   \n",
       "3                                                       \n",
       "4   Scenario: Failed to Create SNDikti Learning Ou...   \n",
       "..                                                ...   \n",
       "68                                                      \n",
       "69                                                      \n",
       "70                                                      \n",
       "71                                                      \n",
       "72                                                      \n",
       "\n",
       "                                           Scenario.3 Scenario.4 Scenario.5  \\\n",
       "0                                                                             \n",
       "1   Scenario: Program Outcomes Export to PDF\\nGive...                         \n",
       "2                                                                             \n",
       "3                                                                             \n",
       "4                                                                             \n",
       "..                                                ...        ...        ...   \n",
       "68                                                                            \n",
       "69                                                                            \n",
       "70                                                                            \n",
       "71                                                                            \n",
       "72                                                                            \n",
       "\n",
       "   Scenario.6 Scenario.7 Scenario.8 Scenario.9 Scenario.10 Effort  \n",
       "0                                                               M  \n",
       "1                                                               M  \n",
       "2                                                               M  \n",
       "3                                                               S  \n",
       "4                                                               M  \n",
       "..        ...        ...        ...        ...         ...    ...  \n",
       "68                                                              S  \n",
       "69                                                              M  \n",
       "70                                                              M  \n",
       "71                                                              M  \n",
       "72                                                              S  \n",
       "\n",
       "[73 rows x 14 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_excel(\"C:/Users/Mirtha/Downloads/Source Code Skripsi/Data Labelling Sendiri.xlsx\")\n",
    "df=df.fillna('')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Menggabungkan data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create Program Outcomes\n",
      "In order to establish Program Outcomes\n",
      "As Curriculum Team,\n",
      "I want the capability to add Program Outcomes\n",
      "Scenario: Create Program Outcomes Successfully\n",
      "Given I am on “http://127.0.0.1:8000/dashboard/curriculum”\n",
      "When I press “Data”\n",
      "And I press “Program Outcomes”\n",
      "Then I should be on “Program Outcomes Page”\n",
      "When I press “Add”\n",
      "Then I should be on “Program Outcomes Creation Page”\n",
      "When I fill in “Program Outcome Code” with “PO1”\n",
      "And I fill in “Program Outcome Description” with “Graduates possess the ability to analyze, design, create, and comprehensively evaluate information systems in alignment with organizational goals, demonstrating effective proficiency.”\n",
      "And I press “Add Program Outcome”\n",
      "Then the response should contain “Success! Program Outcome has been added.”\n",
      "Scenario: Failed to create Program Outcomes as the Program Outcome Code has already been taken.\n",
      "Given I am on “http://127.0.0.1:8000/dashboard/curriculum”\n",
      "When I press “Data”\n",
      "And I press “Program Outcomes”\n",
      "Then I should be on “Program Outcomes Page”\n",
      "And the “Program Outcome Code” field should contain “PO1”\n",
      "When I press “Add”\n",
      "Then I should be on “Program Outcomes Creation Page”\n",
      "When I fill in “Program Outcome Code” with “PO1”\n",
      "And I fill in “Program Outcome Description” with “Graduates possess the ability to analyze, design, create, and comprehensively evaluate information systems in alignment with organizational goals, demonstrating effective proficiency.”\n",
      "And I press “Add Program Outcome”\n",
      "Then the response should contain “The Program Outcome Code has already been taken.”\n",
      "Scenario: Failed to Create Program Outcomes Due to Empty Data Fields\n",
      "Given I am on “http://127.0.0.1:8000/dashboard/curriculum”\n",
      "When I press “Data”\n",
      "And I press “Program Outcomes”\n",
      "Then I should be on “Program Outcomes Page”\n",
      "And the “Program Outcome Code” field should contain “PO1”\n",
      "When I press “Add”\n",
      "Then I should be on “Program Outcomes Creation Page”\n",
      "When I fill in “Program Outcome Code” with “PO2”\n",
      "And I press “Add Program Outcome”\n",
      "Then the response should contain “Program Outcome Description field is required”.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df['combined'] = df.iloc[:, :13].apply(lambda x: '\\n'.join(x), axis=1)\n",
    "\n",
    "# Menampilkan hasil\n",
    "print(df.loc[0, 'combined'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Menghitung Jumlah Skenario dan Kata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     3\n",
       "1     4\n",
       "2     3\n",
       "3     2\n",
       "4     3\n",
       "     ..\n",
       "68    1\n",
       "69    2\n",
       "70    1\n",
       "71    2\n",
       "72    1\n",
       "Name: number_scenario, Length: 73, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['combined'] = df['combined'].str.lower()\n",
    "df['number_word'] = df['combined'].apply(lambda x: len(x.split()))\n",
    "df['number_scenario'] = df['combined'].apply(lambda x: x.count(\"scenario:\"))\n",
    "df['number_word']\n",
    "df['number_scenario'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalisasi Fitur Numerik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number_word</th>\n",
       "      <th>number_scenario</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.066929</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.024833</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.065112</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.013325</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.104179</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>0.058752</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>0.178377</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0.055118</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0.005148</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>73 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    number_word  number_scenario\n",
       "0      0.066929              0.2\n",
       "1      0.024833              0.3\n",
       "2      0.065112              0.2\n",
       "3      0.013325              0.1\n",
       "4      0.104179              0.2\n",
       "..          ...              ...\n",
       "68     0.058752              0.0\n",
       "69     0.178377              0.1\n",
       "70     0.000000              0.0\n",
       "71     0.055118              0.1\n",
       "72     0.005148              0.0\n",
       "\n",
       "[73 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# Inisialisasi MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "# Normalisasi 'number_word' dan 'number_scenario' dengan MinMaxScaler\n",
    "df[['number_word', 'number_scenario']] = scaler.fit_transform(df[['number_word', 'number_scenario']])\n",
    "df[['number_word', 'number_scenario']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create program outcomes\n",
      "in order to establish program outcomes\n",
      "as curriculum team \n",
      "i want the capability to add program outcomes\n",
      "scenario  create program outcomes successfully\n",
      "given i am on  http   127 0 0 1 8000 dashboard curriculum \n",
      "when i press  data \n",
      "and i press  program outcomes \n",
      "then i should be on  program outcomes page \n",
      "when i press  add \n",
      "then i should be on  program outcomes creation page \n",
      "when i fill in  program outcome code  with  po1 \n",
      "and i fill in  program outcome description  with  graduates possess the ability to analyze  design  create  and comprehensively evaluate information systems in alignment with organizational goals  demonstrating effective proficiency  \n",
      "and i press  add program outcome \n",
      "then the response should contain  success  program outcome has been added  \n",
      "scenario  failed to create program outcomes as the program outcome code has already been taken \n",
      "given i am on  http   127 0 0 1 8000 dashboard curriculum \n",
      "when i press  data \n",
      "and i press  program outcomes \n",
      "then i should be on  program outcomes page \n",
      "and the  program outcome code  field should contain  po1 \n",
      "when i press  add \n",
      "then i should be on  program outcomes creation page \n",
      "when i fill in  program outcome code  with  po1 \n",
      "and i fill in  program outcome description  with  graduates possess the ability to analyze  design  create  and comprehensively evaluate information systems in alignment with organizational goals  demonstrating effective proficiency  \n",
      "and i press  add program outcome \n",
      "then the response should contain  the program outcome code has already been taken  \n",
      "scenario  failed to create program outcomes due to empty data fields\n",
      "given i am on  http   127 0 0 1 8000 dashboard curriculum \n",
      "when i press  data \n",
      "and i press  program outcomes \n",
      "then i should be on  program outcomes page \n",
      "and the  program outcome code  field should contain  po1 \n",
      "when i press  add \n",
      "then i should be on  program outcomes creation page \n",
      "when i fill in  program outcome code  with  po2 \n",
      "and i press  add program outcome \n",
      "then the response should contain  program outcome description field is required  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "# Function to remove punctuation\n",
    "def remove_punctuation(text):\n",
    "    punctuation_to_replace = string.punctuation + \"‘’“”\"\n",
    "    translation_table = str.maketrans(punctuation_to_replace, ' ' * len(punctuation_to_replace))\n",
    "    cleaned_text = text.translate(translation_table)\n",
    "    return cleaned_text\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "df['combined'] = df['combined'].apply(remove_punctuation)\n",
    "print(df.loc[0, 'combined'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Mirtha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Mirtha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Mirtha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     [create, program, outcomes, in, order, to, est...\n",
      "1     [read, program, outcomes, in, order, to, evalu...\n",
      "2     [update, program, outcomes, in, order, to, imp...\n",
      "3     [delete, program, outcomes, in, order, to, rem...\n",
      "4     [create, sndikti, learning, outcomes, in, orde...\n",
      "                            ...                        \n",
      "68    [read, study, program, in, order, to, verify, ...\n",
      "69    [update, study, program, in, order, to, improv...\n",
      "70    [read, user, management, in, order, to, verify...\n",
      "71    [update, user, management, in, order, to, impr...\n",
      "72    [delete, user, management, in, order, to, remo...\n",
      "Name: combined, Length: 73, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Load English stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Lemmatizer initialization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Tokenize the text in the 'combined' column\n",
    "df['combined'] = df['combined'].apply(lambda x: word_tokenize(x))\n",
    "print(df['combined'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stopwords Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     [create, program, outcomes, order, establish, ...\n",
       "1     [read, program, outcomes, order, evaluate, pro...\n",
       "2     [update, program, outcomes, order, improve, pr...\n",
       "3     [delete, program, outcomes, order, remove, irr...\n",
       "4     [create, sndikti, learning, outcomes, order, a...\n",
       "                            ...                        \n",
       "68    [read, study, program, order, verify, study, p...\n",
       "69    [update, study, program, order, improve, study...\n",
       "70    [read, user, management, order, verify, user, ...\n",
       "71    [update, user, management, order, improve, use...\n",
       "72    [delete, user, management, order, remove, irre...\n",
       "Name: combined, Length: 73, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to remove stopwords\n",
    "def remove_stopwords(tokens):\n",
    "    return [token for token in tokens if token.lower() not in stop_words]\n",
    "\n",
    "# Remove stopwords from the tokenized text\n",
    "df['combined'] = df['combined'].apply(remove_stopwords)\n",
    "df['combined']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lematisasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     [create, program, outcome, order, establish, p...\n",
       "1     [read, program, outcome, order, evaluate, prog...\n",
       "2     [update, program, outcome, order, improve, pro...\n",
       "3     [delete, program, outcome, order, remove, irre...\n",
       "4     [create, sndikti, learning, outcome, order, ad...\n",
       "                            ...                        \n",
       "68    [read, study, program, order, verify, study, p...\n",
       "69    [update, study, program, order, improve, study...\n",
       "70    [read, user, management, order, verify, user, ...\n",
       "71    [update, user, management, order, improve, use...\n",
       "72    [delete, user, management, order, remove, irre...\n",
       "Name: combined, Length: 73, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define stopwords and lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to perform lemmatization\n",
    "def lemmatize_tokens(tokens):\n",
    "    return [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "df['combined'] = df['combined'].apply(lemmatize_tokens)\n",
    "df['combined']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>100</th>\n",
       "      <th>105</th>\n",
       "      <th>127</th>\n",
       "      <th>197101042008121001</th>\n",
       "      <th>197102111997021001</th>\n",
       "      <th>198105082005012001</th>\n",
       "      <th>198110282006041003</th>\n",
       "      <th>1x3x50</th>\n",
       "      <th>2002</th>\n",
       "      <th>2024</th>\n",
       "      <th>...</th>\n",
       "      <th>want</th>\n",
       "      <th>week</th>\n",
       "      <th>weekly</th>\n",
       "      <th>weight</th>\n",
       "      <th>well</th>\n",
       "      <th>wichern</th>\n",
       "      <th>within</th>\n",
       "      <th>workshop</th>\n",
       "      <th>world</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.033084</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011028</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.078538</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019635</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.063106</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010518</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.093726</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023432</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.018229</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006076</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.047611</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.040811</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.013892</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013892</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.054427</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.024021</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006005</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.047054</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.030501</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.107129</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030501</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.047061</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.247937</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011765</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.051352</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.180362</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025676</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>73 rows × 488 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    100  105       127  197101042008121001  197102111997021001  \\\n",
       "0   0.0  0.0  0.033084                 0.0                 0.0   \n",
       "1   0.0  0.0  0.078538                 0.0                 0.0   \n",
       "2   0.0  0.0  0.063106                 0.0                 0.0   \n",
       "3   0.0  0.0  0.093726                 0.0                 0.0   \n",
       "4   0.0  0.0  0.018229                 0.0                 0.0   \n",
       "..  ...  ...       ...                 ...                 ...   \n",
       "68  0.0  0.0  0.013892                 0.0                 0.0   \n",
       "69  0.0  0.0  0.024021                 0.0                 0.0   \n",
       "70  0.0  0.0  0.030501                 0.0                 0.0   \n",
       "71  0.0  0.0  0.047061                 0.0                 0.0   \n",
       "72  0.0  0.0  0.051352                 0.0                 0.0   \n",
       "\n",
       "    198105082005012001  198110282006041003  1x3x50  2002  2024  ...      want  \\\n",
       "0                  0.0            0.000000     0.0   0.0   0.0  ...  0.011028   \n",
       "1                  0.0            0.000000     0.0   0.0   0.0  ...  0.019635   \n",
       "2                  0.0            0.000000     0.0   0.0   0.0  ...  0.010518   \n",
       "3                  0.0            0.000000     0.0   0.0   0.0  ...  0.023432   \n",
       "4                  0.0            0.000000     0.0   0.0   0.0  ...  0.006076   \n",
       "..                 ...                 ...     ...   ...   ...  ...       ...   \n",
       "68                 0.0            0.000000     0.0   0.0   0.0  ...  0.013892   \n",
       "69                 0.0            0.000000     0.0   0.0   0.0  ...  0.006005   \n",
       "70                 0.0            0.107129     0.0   0.0   0.0  ...  0.030501   \n",
       "71                 0.0            0.247937     0.0   0.0   0.0  ...  0.011765   \n",
       "72                 0.0            0.180362     0.0   0.0   0.0  ...  0.025676   \n",
       "\n",
       "    week  weekly  weight      well  wichern    within  workshop  world  year  \n",
       "0    0.0     0.0     0.0  0.000000      0.0  0.000000       0.0    0.0   0.0  \n",
       "1    0.0     0.0     0.0  0.000000      0.0  0.000000       0.0    0.0   0.0  \n",
       "2    0.0     0.0     0.0  0.000000      0.0  0.000000       0.0    0.0   0.0  \n",
       "3    0.0     0.0     0.0  0.000000      0.0  0.000000       0.0    0.0   0.0  \n",
       "4    0.0     0.0     0.0  0.047611      0.0  0.040811       0.0    0.0   0.0  \n",
       "..   ...     ...     ...       ...      ...       ...       ...    ...   ...  \n",
       "68   0.0     0.0     0.0  0.054427      0.0  0.000000       0.0    0.0   0.0  \n",
       "69   0.0     0.0     0.0  0.047054      0.0  0.000000       0.0    0.0   0.0  \n",
       "70   0.0     0.0     0.0  0.000000      0.0  0.000000       0.0    0.0   0.0  \n",
       "71   0.0     0.0     0.0  0.000000      0.0  0.000000       0.0    0.0   0.0  \n",
       "72   0.0     0.0     0.0  0.000000      0.0  0.000000       0.0    0.0   0.0  \n",
       "\n",
       "[73 rows x 488 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Convert lemmatized tokens back to strings\n",
    "df['combined'] = df['combined'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# Initialize TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the lemmatized text\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df['combined'])\n",
    "\n",
    "# Convert the TF-IDF matrix to a DataFrame\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "tfidf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>100</th>\n",
       "      <th>105</th>\n",
       "      <th>127</th>\n",
       "      <th>197101042008121001</th>\n",
       "      <th>197102111997021001</th>\n",
       "      <th>198105082005012001</th>\n",
       "      <th>198110282006041003</th>\n",
       "      <th>1x3x50</th>\n",
       "      <th>2002</th>\n",
       "      <th>2024</th>\n",
       "      <th>...</th>\n",
       "      <th>weight</th>\n",
       "      <th>well</th>\n",
       "      <th>wichern</th>\n",
       "      <th>within</th>\n",
       "      <th>workshop</th>\n",
       "      <th>world</th>\n",
       "      <th>year</th>\n",
       "      <th>number_word</th>\n",
       "      <th>number_scenario</th>\n",
       "      <th>Effort</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.033084</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.066929</td>\n",
       "      <td>0.2</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.078538</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.024833</td>\n",
       "      <td>0.3</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.063106</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.065112</td>\n",
       "      <td>0.2</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.093726</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.013325</td>\n",
       "      <td>0.1</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.018229</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.047611</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.040811</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.104179</td>\n",
       "      <td>0.2</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.013892</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.054427</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.058752</td>\n",
       "      <td>0.0</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.024021</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.047054</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.178377</td>\n",
       "      <td>0.1</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.030501</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.107129</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.047061</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.247937</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.055118</td>\n",
       "      <td>0.1</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.051352</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.180362</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005148</td>\n",
       "      <td>0.0</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>73 rows × 491 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    100  105       127  197101042008121001  197102111997021001  \\\n",
       "0   0.0  0.0  0.033084                 0.0                 0.0   \n",
       "1   0.0  0.0  0.078538                 0.0                 0.0   \n",
       "2   0.0  0.0  0.063106                 0.0                 0.0   \n",
       "3   0.0  0.0  0.093726                 0.0                 0.0   \n",
       "4   0.0  0.0  0.018229                 0.0                 0.0   \n",
       "..  ...  ...       ...                 ...                 ...   \n",
       "68  0.0  0.0  0.013892                 0.0                 0.0   \n",
       "69  0.0  0.0  0.024021                 0.0                 0.0   \n",
       "70  0.0  0.0  0.030501                 0.0                 0.0   \n",
       "71  0.0  0.0  0.047061                 0.0                 0.0   \n",
       "72  0.0  0.0  0.051352                 0.0                 0.0   \n",
       "\n",
       "    198105082005012001  198110282006041003  1x3x50  2002  2024  ...  weight  \\\n",
       "0                  0.0            0.000000     0.0   0.0   0.0  ...     0.0   \n",
       "1                  0.0            0.000000     0.0   0.0   0.0  ...     0.0   \n",
       "2                  0.0            0.000000     0.0   0.0   0.0  ...     0.0   \n",
       "3                  0.0            0.000000     0.0   0.0   0.0  ...     0.0   \n",
       "4                  0.0            0.000000     0.0   0.0   0.0  ...     0.0   \n",
       "..                 ...                 ...     ...   ...   ...  ...     ...   \n",
       "68                 0.0            0.000000     0.0   0.0   0.0  ...     0.0   \n",
       "69                 0.0            0.000000     0.0   0.0   0.0  ...     0.0   \n",
       "70                 0.0            0.107129     0.0   0.0   0.0  ...     0.0   \n",
       "71                 0.0            0.247937     0.0   0.0   0.0  ...     0.0   \n",
       "72                 0.0            0.180362     0.0   0.0   0.0  ...     0.0   \n",
       "\n",
       "        well  wichern    within  workshop  world  year  number_word  \\\n",
       "0   0.000000      0.0  0.000000       0.0    0.0   0.0     0.066929   \n",
       "1   0.000000      0.0  0.000000       0.0    0.0   0.0     0.024833   \n",
       "2   0.000000      0.0  0.000000       0.0    0.0   0.0     0.065112   \n",
       "3   0.000000      0.0  0.000000       0.0    0.0   0.0     0.013325   \n",
       "4   0.047611      0.0  0.040811       0.0    0.0   0.0     0.104179   \n",
       "..       ...      ...       ...       ...    ...   ...          ...   \n",
       "68  0.054427      0.0  0.000000       0.0    0.0   0.0     0.058752   \n",
       "69  0.047054      0.0  0.000000       0.0    0.0   0.0     0.178377   \n",
       "70  0.000000      0.0  0.000000       0.0    0.0   0.0     0.000000   \n",
       "71  0.000000      0.0  0.000000       0.0    0.0   0.0     0.055118   \n",
       "72  0.000000      0.0  0.000000       0.0    0.0   0.0     0.005148   \n",
       "\n",
       "    number_scenario  Effort  \n",
       "0               0.2       M  \n",
       "1               0.3       M  \n",
       "2               0.2       M  \n",
       "3               0.1       S  \n",
       "4               0.2       M  \n",
       "..              ...     ...  \n",
       "68              0.0       S  \n",
       "69              0.1       M  \n",
       "70              0.0       M  \n",
       "71              0.1       M  \n",
       "72              0.0       S  \n",
       "\n",
       "[73 rows x 491 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenate 'effort' column of the original DataFrame with TF-IDF DataFrame\n",
    "integrated_data = pd.concat([tfidf_df, df[['number_word', 'number_scenario', 'Effort']]], axis=1)\n",
    "integrated_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One hot encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# One-Hot Encoding for 'Effort'\n",
    "onehot_encoder = OneHotEncoder()\n",
    "effort_encoded = onehot_encoder.fit_transform(integrated_data['Effort'].values.reshape(-1,1))\n",
    "effort_encoded_array = effort_encoded.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mengambil indeks kolom dengan nilai maksimum untuk setiap baris\n",
    "effort_labels = np.argmax(effort_encoded_array, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded Categories: [array(['L', 'M', 'S', 'XL', 'XXL'], dtype=object)]\n",
      "Kategori 'L' diencode menjadi: 0\n",
      "Kategori 'M' diencode menjadi: 1\n",
      "Kategori 'S' diencode menjadi: 2\n",
      "Kategori 'XL' diencode menjadi: 3\n",
      "Kategori 'XXL' diencode menjadi: 4\n"
     ]
    }
   ],
   "source": [
    "# Melihat kategori yang diencode\n",
    "encoded_categories = onehot_encoder.categories_\n",
    "print(\"Encoded Categories:\", encoded_categories)\n",
    "\n",
    "# Mencetak nilai yang sesuai untuk setiap kategori\n",
    "for i, category in enumerate(encoded_categories[0]):\n",
    "    print(f\"Kategori '{category}' diencode menjadi:\", i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Mirtha\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Perform one hot encoding on the target variable\n",
    "one_hot_encoder = OneHotEncoder(categories='auto', sparse=False)\n",
    "effort2 = one_hot_encoder.fit_transform(df['Effort'].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(integrated_data.drop(columns=['Effort']), effort2, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [58, 290]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m adaboost_classifier \u001b[38;5;241m=\u001b[39m AdaBoostClassifier(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Train the Adaboost classifier\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[43madaboost_classifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mravel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Menggunakan .ravel() untuk mengubah menjadi array satu dimensi\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Predict on the test set\u001b[39;00m\n\u001b[0;32m      8\u001b[0m y_pred_encoded \u001b[38;5;241m=\u001b[39m adaboost_classifier\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "File \u001b[1;32mc:\\Users\\Mirtha\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Mirtha\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:135\u001b[0m, in \u001b[0;36mBaseWeightBoosting.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(\n\u001b[0;32m    111\u001b[0m     \u001b[38;5;66;03m# AdaBoost*.estimator is not validated yet\u001b[39;00m\n\u001b[0;32m    112\u001b[0m     prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    113\u001b[0m )\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    115\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build a boosted classifier/regressor from the training set (X, y).\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 135\u001b[0m     X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    136\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    137\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    138\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    139\u001b[0m \u001b[43m        \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    140\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    142\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_numeric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_regressor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    143\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    145\u001b[0m     sample_weight \u001b[38;5;241m=\u001b[39m _check_sample_weight(\n\u001b[0;32m    146\u001b[0m         sample_weight, X, np\u001b[38;5;241m.\u001b[39mfloat64, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, only_non_negative\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    147\u001b[0m     )\n\u001b[0;32m    148\u001b[0m     sample_weight \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m sample_weight\u001b[38;5;241m.\u001b[39msum()\n",
      "File \u001b[1;32mc:\\Users\\Mirtha\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:622\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    620\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[0;32m    621\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 622\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    623\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[1;32mc:\\Users\\Mirtha\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:1164\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1146\u001b[0m X \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[0;32m   1147\u001b[0m     X,\n\u001b[0;32m   1148\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1159\u001b[0m     input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1160\u001b[0m )\n\u001b[0;32m   1162\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[1;32m-> 1164\u001b[0m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X, y\n",
      "File \u001b[1;32mc:\\Users\\Mirtha\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:407\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    405\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[0;32m    406\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 407\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    408\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    409\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[0;32m    410\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [58, 290]"
     ]
    }
   ],
   "source": [
    "# Initialize Adaboost classifier\n",
    "adaboost_classifier = AdaBoostClassifier(n_estimators=50, random_state=42)\n",
    "\n",
    "# Train the Adaboost classifier\n",
    "adaboost_classifier.fit(X_train, y_train.ravel())  # Menggunakan .ravel() untuk mengubah menjadi array satu dimensi\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_encoded = adaboost_classifier.predict(X_test)\n",
    "\n",
    "# Convert encoded predictions back to original labels\n",
    "y_pred = one_hot_encoder.inverse_transform(y_pred_encoded)\n",
    "\n",
    "# Evaluate the classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data into features (X) and target variable (y)\n",
    "X = integrated_data.drop(columns=['Effort'])\n",
    "y = effort_encoded_array\n",
    "\n",
    "# Splitting data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "y should be a 1d array, got an array of shape (58, 5) instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# AdaBoost Classifier\u001b[39;00m\n\u001b[0;32m      2\u001b[0m adaboost_model \u001b[38;5;241m=\u001b[39m AdaBoostClassifier()\n\u001b[1;32m----> 3\u001b[0m \u001b[43madaboost_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m adaboost_pred \u001b[38;5;241m=\u001b[39m adaboost_model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[0;32m      5\u001b[0m adaboost_accuracy \u001b[38;5;241m=\u001b[39m accuracy_score(y_test, adaboost_pred)\n",
      "File \u001b[1;32mc:\\Users\\Mirtha\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Mirtha\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:135\u001b[0m, in \u001b[0;36mBaseWeightBoosting.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(\n\u001b[0;32m    111\u001b[0m     \u001b[38;5;66;03m# AdaBoost*.estimator is not validated yet\u001b[39;00m\n\u001b[0;32m    112\u001b[0m     prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    113\u001b[0m )\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    115\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build a boosted classifier/regressor from the training set (X, y).\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 135\u001b[0m     X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    136\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    137\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    138\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    139\u001b[0m \u001b[43m        \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    140\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    142\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_numeric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_regressor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    143\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    145\u001b[0m     sample_weight \u001b[38;5;241m=\u001b[39m _check_sample_weight(\n\u001b[0;32m    146\u001b[0m         sample_weight, X, np\u001b[38;5;241m.\u001b[39mfloat64, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, only_non_negative\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    147\u001b[0m     )\n\u001b[0;32m    148\u001b[0m     sample_weight \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m sample_weight\u001b[38;5;241m.\u001b[39msum()\n",
      "File \u001b[1;32mc:\\Users\\Mirtha\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:622\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    620\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[0;32m    621\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 622\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    623\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[1;32mc:\\Users\\Mirtha\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:1162\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1142\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1143\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1144\u001b[0m     )\n\u001b[0;32m   1146\u001b[0m X \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[0;32m   1147\u001b[0m     X,\n\u001b[0;32m   1148\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1159\u001b[0m     input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1160\u001b[0m )\n\u001b[1;32m-> 1162\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43m_check_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmulti_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmulti_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_numeric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_numeric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1164\u001b[0m check_consistent_length(X, y)\n\u001b[0;32m   1166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X, y\n",
      "File \u001b[1;32mc:\\Users\\Mirtha\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:1183\u001b[0m, in \u001b[0;36m_check_y\u001b[1;34m(y, multi_output, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1181\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1182\u001b[0m     estimator_name \u001b[38;5;241m=\u001b[39m _check_estimator_name(estimator)\n\u001b[1;32m-> 1183\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mcolumn_or_1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m   1184\u001b[0m     _assert_all_finite(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, estimator_name\u001b[38;5;241m=\u001b[39mestimator_name)\n\u001b[0;32m   1185\u001b[0m     _ensure_no_complex_data(y)\n",
      "File \u001b[1;32mc:\\Users\\Mirtha\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:1244\u001b[0m, in \u001b[0;36mcolumn_or_1d\u001b[1;34m(y, dtype, warn)\u001b[0m\n\u001b[0;32m   1233\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1234\u001b[0m             (\n\u001b[0;32m   1235\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA column-vector y was passed when a 1d array was\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1240\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m   1241\u001b[0m         )\n\u001b[0;32m   1242\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _asarray_with_order(xp\u001b[38;5;241m.\u001b[39mreshape(y, (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,)), order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m\"\u001b[39m, xp\u001b[38;5;241m=\u001b[39mxp)\n\u001b[1;32m-> 1244\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1245\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my should be a 1d array, got an array of shape \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(shape)\n\u001b[0;32m   1246\u001b[0m )\n",
      "\u001b[1;31mValueError\u001b[0m: y should be a 1d array, got an array of shape (58, 5) instead."
     ]
    }
   ],
   "source": [
    "# AdaBoost Classifier\n",
    "adaboost_model = AdaBoostClassifier()\n",
    "adaboost_model.fit(X_train, y_train)\n",
    "adaboost_pred = adaboost_model.predict(X_test)\n",
    "adaboost_accuracy = accuracy_score(y_test, adaboost_pred)\n",
    "print(\"AdaBoost Accuracy:\", adaboost_accuracy)\n",
    "print(\"AdaBoost Classification Report:\\n\", classification_report(y_test, adaboost_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "# Splitting data into features (X) and target variable (y)\n",
    "X = integrated_data.drop(columns=['Effort'])\n",
    "y = effort_labels\n",
    "\n",
    "# Define AdaBoost Classifier\n",
    "adaboost_model = AdaBoostClassifier()\n",
    "\n",
    "# Perform k-fold cross-validation\n",
    "cv_scores = cross_val_score(adaboost_model, X, y, cv=6)  # cv=5 for 5-fold cross-validation\n",
    "\n",
    "# Print the cross-validation scores\n",
    "print(\"Cross-validation scores:\", cv_scores)\n",
    "print(\"Mean cross-validation score:\", cv_scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Splitting data into features (X) and target variable (y)\n",
    "X = integrated_data.drop(columns=['Effort'])\n",
    "y = effort_labels\n",
    "\n",
    "# Define AdaBoost Classifier\n",
    "adaboost_model = AdaBoostClassifier()\n",
    "\n",
    "# Perform k-fold cross-validation\n",
    "predicted = cross_val_predict(adaboost_model, X, y, cv=6)  # cv=5 for 5-fold cross-validation\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_matrix = confusion_matrix(y, predicted)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adaboost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdaBoost Classifier\n",
    "adaboost_model = AdaBoostClassifier()\n",
    "adaboost_model.fit(X_train, y_train_encoded)\n",
    "adaboost_pred = adaboost_model.predict(X_test)\n",
    "adaboost_accuracy = accuracy_score(y_test_encoded, adaboost_pred)\n",
    "print(\"AdaBoost Accuracy:\", adaboost_accuracy)\n",
    "print(\"AdaBoost Classification Report:\\n\", classification_report(y_test_encoded, adaboost_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Membuat daftar lengkap kelas\n",
    "full_class_list = np.unique(np.concatenate((y_train_encoded, y_test_encoded)))\n",
    "\n",
    "# Menghitung confusion matrix dengan daftar lengkap kelas\n",
    "conf_matrix_adaboost = confusion_matrix(y_test_encoded, adaboost_pred, labels=full_class_list)\n",
    "\n",
    "# Visualisasi confusion matrix sebagai gambar\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix_adaboost, annot=True, fmt=\"d\", cmap=\"Blues\", \n",
    "            xticklabels=full_class_list,\n",
    "            yticklabels=full_class_list)\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix for AdaBoost Classifier')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree Classifier\n",
    "decision_tree_model = DecisionTreeClassifier()\n",
    "decision_tree_model.fit(X_train, y_train_encoded)\n",
    "decision_tree_pred = decision_tree_model.predict(X_test)\n",
    "decision_tree_accuracy = accuracy_score(y_test_encoded, decision_tree_pred)\n",
    "print(\"Decision Tree Accuracy:\", decision_tree_accuracy)\n",
    "print(\"Decision Tree Classification Report:\\n\", classification_report(y_test_encoded, decision_tree_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "\n",
    "# Visualisasi decision tree\n",
    "plt.figure(figsize=(20,10))\n",
    "plot_tree(decision_tree_model, filled=True, feature_names=X_train.columns, class_names=np.unique(y_train_encoded).astype(str))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menghitung confusion matrix dengan daftar lengkap kelas\n",
    "conf_matrix_decision_tree = confusion_matrix(y_test_encoded, decision_tree_pred, labels=full_class_list)\n",
    "\n",
    "# Visualisasi confusion matrix sebagai gambar\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix_decision_tree, annot=True, fmt=\"d\", cmap=\"Blues\", \n",
    "            xticklabels=full_class_list,\n",
    "            yticklabels=full_class_list)\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix for Decision Tree Classifier')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multinomial Naive Bayes Classifier\n",
    "naive_bayes_model = MultinomialNB()\n",
    "naive_bayes_model.fit(X_train, y_train_encoded)\n",
    "naive_bayes_pred = naive_bayes_model.predict(X_test)\n",
    "naive_bayes_accuracy = accuracy_score(y_test_encoded, naive_bayes_pred)\n",
    "print(\"Multinomial Naive Bayes Accuracy:\", naive_bayes_accuracy)\n",
    "print(\"Multinomial Naive Bayes Classification Report:\\n\", classification_report(y_test_encoded, naive_bayes_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menghitung confusion matrix dengan daftar lengkap kelas\n",
    "conf_matrix_naive_bayes = confusion_matrix(y_test_encoded, naive_bayes_pred, labels=full_class_list)\n",
    "\n",
    "# Visualisasi confusion matrix sebagai gambar\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix_naive_bayes, annot=True, fmt=\"d\", cmap=\"Blues\", \n",
    "            xticklabels=full_class_list,\n",
    "            yticklabels=full_class_list)\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix for Multinomial Naive Bayes Classifier')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediksi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contoh paragraf yang ingin diprediksi\n",
    "paragraph = \"\"\"Create Program Outcomes\t\"In order to establish Program Outcomes\n",
    "As Curriculum Team,\n",
    "I want the capability to add Program Outcomes\"\t\"Scenario: Create Program Outcomes Successfully\n",
    "Given I am on “http://127.0.0.1:8000/dashboard/curriculum”\n",
    "When I press “Data”\n",
    "And I press “Program Outcomes”\n",
    "Then I should be on “Program Outcomes Page”\n",
    "When I press “Add”\n",
    "Then I should be on “Program Outcomes Creation Page”\n",
    "When I fill in “Program Outcome Code” with “PO1”\n",
    "And I fill in “Program Outcome Description” with “Graduates possess the ability to analyze, design, create, and comprehensively evaluate information systems in alignment with organizational goals, demonstrating effective proficiency.”\n",
    "And I press “Add Program Outcome”\n",
    "Then the response should contain “Success! Program Outcome has been added.”\"\t\"Scenario: Failed to create Program Outcomes as the Program Outcome Code has already been taken.\n",
    "Given I am on “http://127.0.0.1:8000/dashboard/curriculum”\n",
    "When I press “Data”\n",
    "And I press “Program Outcomes”\n",
    "Then I should be on “Program Outcomes Page”\n",
    "And the “Program Outcome Code” field should contain “PO1”\n",
    "When I press “Add”\n",
    "Then I should be on “Program Outcomes Creation Page”\n",
    "When I fill in “Program Outcome Code” with “PO1”\n",
    "And I fill in “Program Outcome Description” with “Graduates possess the ability to analyze, design, create, and comprehensively evaluate information systems in alignment with organizational goals, demonstrating effective proficiency.”\n",
    "And I press “Add Program Outcome”\n",
    "Then the response should contain “The Program Outcome Code has already been taken.”\"\t\"Scenario: Failed to Create Program Outcomes Due to Empty Data Fields\n",
    "Given I am on “http://127.0.0.1:8000/dashboard/curriculum”\n",
    "When I press “Data”\n",
    "And I press “Program Outcomes”\n",
    "Then I should be on “Program Outcomes Page”\n",
    "And the “Program Outcome Code” field should contain “PO1”\n",
    "When I press “Add”\n",
    "Then I should be on “Program Outcomes Creation Page”\n",
    "When I fill in “Program Outcome Code” with “PO2”\n",
    "And I press “Add Program Outcome”\n",
    "Then the response should contain “Program Outcome Description field is required”.\"\"\"\n",
    "\n",
    "# Pra-pemrosesan teks (misalnya tokenisasi, menghapus stopwords, normalisasi)\n",
    "# ...\n",
    "\n",
    "# Vektorisasi paragraf\n",
    "paragraph_vectorized = vectorizer.transform([paragraph])  # Menggunakan vektorisasi yang sama yang digunakan pada data latih\n",
    "\n",
    "# Prediksi dengan model AdaBoost\n",
    "predicted_label = adaboost_model.predict(paragraph_vectorized)\n",
    "\n",
    "# Keluarkan hasil prediksi\n",
    "print(\"Hasil Prediksi untuk Paragraf:\")\n",
    "print(predicted_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data into features (X) and target variable (y)\n",
    "X = integrated_data.drop(columns=['Effort'])\n",
    "y = integrated_data['Effort']\n",
    "\n",
    "# Splitting data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# Perform one-hot encoding on the target variable\n",
    "onehot_encoder = OneHotEncoder()\n",
    "y_train_encoded = onehot_encoder.fit_transform(y_train.values.reshape(-1,1)).toarray()  # Convert to dense array\n",
    "y_test_encoded = onehot_encoder.transform(y_test.values.reshape(-1,1)).toarray()  # Convert to dense array\n",
    "\n",
    "# Train AdaBoost model\n",
    "adaboost_model = AdaBoostClassifier()\n",
    "adaboost_model.fit(X_train, y_train_encoded)\n",
    "\n",
    "# Predict\n",
    "y_pred = adaboost_model.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "accuracy = accuracy_score(y_test_encoded, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\\n\", classification_report(y_test_encoded, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# One-Hot Encoding for 'Effort'\n",
    "onehot_encoder = OneHotEncoder()\n",
    "effort_encoded = onehot_encoder.fit_transform(integrated_data['Effort'].values.reshape(-1,1))\n",
    "\n",
    "# Splitting data into features (X) and target variable (y)\n",
    "X = integrated_data.drop(columns=['Effort'])\n",
    "y = effort_encoded\n",
    "\n",
    "# Splitting data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Model Creation and Evaluation\n",
    "\n",
    "# AdaBoost Classifier\n",
    "adaboost_model = AdaBoostClassifier()\n",
    "adaboost_model.fit(X_train, y_train)\n",
    "adaboost_pred = adaboost_model.predict(X_test)\n",
    "adaboost_accuracy = accuracy_score(y_test, adaboost_pred)\n",
    "print(\"AdaBoost Accuracy:\", adaboost_accuracy)\n",
    "print(\"AdaBoost Classification Report:\\n\", classification_report(y_test, adaboost_pred))\n",
    "\n",
    "# Decision Tree Classifier\n",
    "decision_tree_model = DecisionTreeClassifier()\n",
    "decision_tree_model.fit(X_train, y_train)\n",
    "decision_tree_pred = decision_tree_model.predict(X_test)\n",
    "decision_tree_accuracy = accuracy_score(y_test, decision_tree_pred)\n",
    "print(\"Decision Tree Accuracy:\", decision_tree_accuracy)\n",
    "print(\"Decision Tree Classification Report:\\n\", classification_report(y_test, decision_tree_pred))\n",
    "\n",
    "# Multinomial Naive Bayes Classifier\n",
    "naive_bayes_model = MultinomialNB()\n",
    "naive_bayes_model.fit(X_train, y_train)\n",
    "naive_bayes_pred = naive_bayes_model.predict(X_test)\n",
    "naive_bayes_accuracy = accuracy_score(y_test, naive_bayes_pred)\n",
    "print(\"Multinomial Naive Bayes Accuracy:\", naive_bayes_accuracy)\n",
    "print(\"Multinomial Naive Bayes Classification Report:\\n\", classification_report(y_test, naive_bayes_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Case Folding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Feature']=df['Feature'].str.lower()\n",
    "df['User Story']=df['User Story'].str.lower()\n",
    "df['Normal Flow']=df['Normal Flow'].str.lower()\n",
    "df['Exception Flow']=df['Exception Flow'].str.lower()\n",
    "df['Alternatif Flow']=df['Alternatif Flow'].str.lower()\n",
    "label_mapping = {'Mudah': 0, 'Cukup': 1, 'Sulit': 2}\n",
    "df['Label_Numeric'] = df['Score'].map(label_mapping)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove URL, html, dan punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def remove_URL(text):\n",
    "    def replace_url(match):\n",
    "        return \"\"\n",
    "\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url_pattern.sub(replace_url, text)\n",
    "\n",
    "# ternyata tidak ada sepertinya\n",
    "def remove_html(text): \n",
    "    def replace_html(match):\n",
    "        return \"\"\n",
    "    html=re.compile(r'<.*?>')\n",
    "    return html.sub(replace_html,text)\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    table=str.maketrans('','',string.punctuation)\n",
    "    return text.translate(table)\n",
    "\n",
    "selected_columns = ['Feature', 'User Story', 'Normal Flow', 'Exception Flow', 'Alternatif Flow']\n",
    "\n",
    "for col in selected_columns:\n",
    "    for i in range(len(df[col])):\n",
    "        df.at[i, col] = remove_URL(df.at[i, col])\n",
    "        df.at[i, col] = remove_html(df.at[i, col])\n",
    "        df.at[i, col] = remove_punctuation(df.at[i, col])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spelling Correction (JANGAN DULU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spellchecker import SpellChecker\n",
    "\n",
    "spell = SpellChecker()\n",
    "\n",
    "def correct_spellings(text):\n",
    "    corrected_text = []\n",
    "    misspelled_words = spell.unknown(text.split())\n",
    "    for word in text.split():\n",
    "        if word in misspelled_words:\n",
    "            corrected_word = spell.correction(word)\n",
    "            if corrected_word is not None:\n",
    "                corrected_text.append(corrected_word)\n",
    "            else:\n",
    "                # Handle the case where correction is None\n",
    "                corrected_text.append(word)\n",
    "        else:\n",
    "            corrected_text.append(word)\n",
    "    return \" \".join(corrected_text)\n",
    "\n",
    "for col in selected_columns:\n",
    "    for i in range(len(df[col])):\n",
    "        df.at[i, col] = correct_spellings(df.at[i, col])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"corect me plese\"\n",
    "correct_spellings(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "selected_columns = ['Feature', 'User Story', 'Normal Flow', 'Exception Flow', 'Alternatif Flow']\n",
    "\n",
    "for col in selected_columns:\n",
    "    for i in range(len(df[col])):\n",
    "        df.at[i, col] = nltk.word_tokenize(df.at[i, col])\n",
    "\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in selected_columns:\n",
    "    for i in range(len(df[col])):\n",
    "        first_text =  df.at[i, col]\n",
    "        first_text_list_cleaned = [word for word in first_text if word.lower() not in stopwords]\n",
    "        df.at[i, col] = first_text_list_cleaned\n",
    "df\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemm = WordNetLemmatizer()\n",
    "\n",
    "for col in selected_columns:\n",
    "    for i in range(len(df[col])):\n",
    "        # Tokenize each text\n",
    "        tokens = df.at[i, col]\n",
    "        # Lemmatize each word\n",
    "        lemmatized_tokens = [lemm.lemmatize(word) for word in tokens]\n",
    "        # Join the lemmatized tokens back into a string\n",
    "        lemmatized_text = ' '.join(lemmatized_tokens)\n",
    "        # Update the DataFrame with the lemmatized text\n",
    "        df.at[i, col] = lemmatized_text\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualisasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "from io import BytesIO\n",
    "from PIL import Image  # Tambahkan baris ini\n",
    "import numpy as np\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "url = \"https://user-images.githubusercontent.com/74188336/142692890-641ebc21-2e47-4556-9d37-1c0b9e1a0587.jpeg\"\n",
    "response = requests.get(url)\n",
    "img = Image.open(BytesIO(response.content))\n",
    "\n",
    "text = ' '.join(df['Feature'].values)\n",
    "mask = np.array(img)\n",
    "wordcloud = WordCloud(max_font_size=50, max_words=1000, background_color=\"white\", mask=mask, colormap='BuGn').generate(text.lower())\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://user-images.githubusercontent.com/74188336/142692894-c17240e4-1101-4591-9d10-71793e460816.jpeg\"\n",
    "response = requests.get(url)\n",
    "img = Image.open(BytesIO(response.content))\n",
    "\n",
    "text = ' '.join(df['User Story'].values)\n",
    "mask = np.array(img)\n",
    "wordcloud = WordCloud(max_font_size=50, max_words=1000, background_color=\"white\", mask=mask, colormap='BuGn').generate(text.lower())\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Create and train the Multinomial Logistic Regression model\n",
    "multinomial_model = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
    "# Gunakan cross_val_predict untuk mendapatkan label prediksi untuk setiap lipatan cross-validation\n",
    "y_pred_multinomial = cross_val_predict(multinomial_model, X_combined, y, cv=stratified_kfold)\n",
    "\n",
    "# Hitung confusion matrix\n",
    "conf_matrix_multinomial = confusion_matrix(y, y_pred_multinomial)\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix_multinomial)\n",
    "\n",
    "# Visualisasikan confusion matrix jika diperlukan\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix_multinomial, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Class 1\", \"Class 2\", \"Class 3\"], yticklabels=[\"Class 1\", \"Class 2\", \"Class 3\"])\n",
    "plt.title('Confusion Matrix (Cross-Validation)')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.show()\n",
    "\n",
    "# Additional evaluation metrics\n",
    "accuracy_multinomial = accuracy_score(y, y_pred_multinomial)\n",
    "print(f\"\\nAccuracy for Multinomial Logistic Regression: {accuracy_multinomial:.2f}\")\n",
    "\n",
    "classification_report_multinomial = classification_report(y, y_pred_multinomial)\n",
    "print(\"\\nClassification Report for Multinomial Logistic Regression:\")\n",
    "print(classification_report_multinomial)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "rf_model = RandomForestClassifier()\n",
    "\n",
    "# Gunakan cross_val_predict dengan StratifiedKFold\n",
    "y_pred_rf = cross_val_predict(rf_model, X_combined, y, cv=stratified_kfold)\n",
    "\n",
    "# Hitung confusion matrix\n",
    "conf_matrix_rf = confusion_matrix(y, y_pred_rf)\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix_rf)\n",
    "\n",
    "# Visualisasikan confusion matrix jika diperlukan\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix_rf, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Class 1\", \"Class 2\", \"Class 3\"], yticklabels=[\"Class 1\", \"Class 2\", \"Class 3\"])\n",
    "plt.title('Confusion Matrix (Cross-Validation)')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.show()\n",
    "\n",
    "# Additional evaluation metrics\n",
    "accuracy_rf = accuracy_score(y, y_pred_rf)\n",
    "print(f\"\\nAccuracy for Random Forest: {accuracy_rf:.2f}\")\n",
    "\n",
    "classification_report_rf = classification_report(y, y_pred_rf)\n",
    "print(\"\\nClassification Report for Random Forest:\")\n",
    "print(classification_report_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.model_selection import cross_val_predict, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from scipy.sparse import hstack\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Specify the number of folds (e.g., 5 or 10)\n",
    "num_folds = 5\n",
    "\n",
    "# Create a StratifiedKFold object with the desired number of splits\n",
    "stratified_kfold = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Menggabungkan semua fitur ke dalam matriks tunggal\n",
    "df['Combined_Text'] = df['Feature'] + ' ' + df['User Story'] + ' ' + df['Normal Flow'] + ' ' + df['Exception Flow'] + ' ' + df['Alternatif Flow']\n",
    "X_combined = TfidfVectorizer(max_features=5000).fit_transform(df['Combined_Text'])\n",
    "X_combined = hstack([X_combined, df[['Jumlah Scenario', 'Jumlah kata']]])\n",
    "\n",
    "# Memilih kolom-kolom yang akan digunakan sebagai label\n",
    "y = df['Score']\n",
    "\n",
    "# Inisialisasi model Linear SVM, Logistic Regression, dan Random Forest\n",
    "svm_model = LinearSVC()\n",
    "logistic_model = LogisticRegression()\n",
    "rf_model = RandomForestClassifier()\n",
    "\n",
    "# Membuat Stacking Ensemble dengan model-model yang telah diinisialisasi\n",
    "stacking_model = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('svm', svm_model),\n",
    "        ('logistic', logistic_model),\n",
    "        ('random_forest', rf_model)\n",
    "    ],\n",
    "    cv=stratified_kfold\n",
    ")\n",
    "\n",
    "\n",
    "# Gunakan cross_val_predict untuk mendapatkan label prediksi untuk setiap lipatan cross-validation\n",
    "y_pred_stacking = cross_val_predict(stacking_model, X_combined, y.values.ravel(), cv=stratified_kfold)\n",
    "\n",
    "\n",
    "# Hitung confusion matrix\n",
    "conf_matrix_stacking = confusion_matrix(y, y_pred_stacking)\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix_stacking)\n",
    "\n",
    "# Visualisasikan confusion matrix jika diperlukan\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix_stacking, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Class 1\", \"Class 2\", \"Class 3\"], yticklabels=[\"Class 1\", \"Class 2\", \"Class 3\"])\n",
    "plt.title('Confusion Matrix (Stacking Ensemble)')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.show()\n",
    "\n",
    "# Additional evaluation metrics\n",
    "accuracy_stacking = accuracy_score(y, y_pred_stacking)\n",
    "print(f\"\\nAccuracy for Stacking Ensemble: {accuracy_stacking:.2f}\")\n",
    "\n",
    "classification_report_stacking = classification_report(y, y_pred_stacking)\n",
    "print(\"\\nClassification Report for Stacking Ensemble:\")\n",
    "print(classification_report_stacking)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "# Stacking classifier without final_estimator\n",
    "stacking_model = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('svm', svm_model),\n",
    "        ('lr', logistic_model),\n",
    "        ('rf', rf_model)\n",
    "    ],\n",
    ")\n",
    "\n",
    "y_pred_stacking = cross_val_predict(stacking_model, X_combined, y.values.ravel(), cv=stratified_kfold)\n",
    "\n",
    "# Hitung confusion matrix\n",
    "conf_matrix_stacking = confusion_matrix(y, y_pred_stacking)\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix_stacking)\n",
    "\n",
    "# Visualisasikan confusion matrix jika diperlukan\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix_stacking, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Class 1\", \"Class 2\", \"Class 3\"], yticklabels=[\"Class 1\", \"Class 2\", \"Class 3\"])\n",
    "plt.title('Confusion Matrix (Cross-Validation)')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.show()\n",
    "\n",
    "# Additional evaluation metrics\n",
    "accuracy_stacking = accuracy_score(y, y_pred_stacking)\n",
    "print(f\"\\nAccuracy for Ensemble Stacking Model: {accuracy_stacking:.2f}\")\n",
    "\n",
    "classification_report_stacking = classification_report(y, y_pred_stacking)\n",
    "print(\"\\nClassification Report for Ensemble Stacking Model:\")\n",
    "print(classification_report_stacking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from scipy.sparse import hstack\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Menggunakan MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "df['Combined_Text'] = df['Feature'] + ' ' + df['User Story'] + ' ' + df['Normal Flow'] + ' ' + df['Exception Flow'] + ' ' + df['Alternatif Flow']\n",
    "\n",
    "# Membagi dataset menjadi data latih dan data uji\n",
    "train, test = train_test_split(df, test_size=0.4)\n",
    "\n",
    "# Memilih kolom-kolom yang akan digunakan sebagai fitur dan label\n",
    "train_X = train[['Combined_Text']]\n",
    "train_numeric = scaler.fit_transform(train[['Jumlah Scenario', 'Jumlah kata']])\n",
    "train_y = train['Score']\n",
    "test_X = test[['Combined_Text']]\n",
    "test_y = test['Score']\n",
    "\n",
    "# Feature extraction (TF-IDF) for each column on training set\n",
    "tfidf_vectorizer_user_story = TfidfVectorizer(max_features=5000)\n",
    "X_train = tfidf_vectorizer_user_story.fit_transform(train_X['Combined_Text'])\n",
    "\n",
    "# Combine training set matrices\n",
    "X_train_combined = hstack([X_train, train_numeric])\n",
    "\n",
    "# Feature extraction (TF-IDF) for each column on test set\n",
    "X_test = tfidf_vectorizer_user_story.transform(test_X['Combined_Text'])\n",
    "\n",
    "# Combine test set matrices\n",
    "X_test_combined = hstack([X_test, scaler.transform(test[['Jumlah Scenario', 'Jumlah kata']])])\n",
    "\n",
    "# Model menggunakan LinearSVC\n",
    "svm_model = LinearSVC()  \n",
    "svm_model.fit(X_train_combined, train_y)\n",
    "y_pred_svm = svm_model.predict(X_test_combined)\n",
    "\n",
    "# Confusion Matrix for SVM\n",
    "conf_matrix_svm = confusion_matrix(test_y, y_pred_svm)\n",
    "\n",
    "# Plotting the heatmap for SVM\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix_svm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Class 1\", \"Class 2\", \"Class 3\"], yticklabels=[\"Class 1\", \"Class 2\", \"Class 3\"])\n",
    "plt.title('Confusion Matrix for Linear SVM')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.show()\n",
    "\n",
    "# Model menggunakan Adaboost dengan pengaturan algorithm='SAMME'\n",
    "adaboost_model = AdaBoostClassifier(base_estimator=LinearSVC(), n_estimators=50, algorithm='SAMME', random_state=42)\n",
    "adaboost_model.fit(X_train_combined, train_y)\n",
    "y_pred_adaboost = adaboost_model.predict(X_test_combined)\n",
    "\n",
    "# Confusion Matrix for Adaboost\n",
    "conf_matrix_adaboost = confusion_matrix(test_y, y_pred_adaboost)\n",
    "\n",
    "# Plotting the heatmap for Adaboost\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix_adaboost, annot=True, fmt=\"d\", cmap=\"Greens\", xticklabels=[\"Class 1\", \"Class 2\", \"Class 3\"], yticklabels=[\"Class 1\", \"Class 2\", \"Class 3\"])\n",
    "plt.title('Confusion Matrix for Adaboost with LinearSVC base learner (algorithm= SAMME)')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Define your models\n",
    "svm_model = LinearSVC()\n",
    "adaboost_model = AdaBoostClassifier(base_estimator=LinearSVC(), n_estimators=50, algorithm='SAMME', random_state=42)\n",
    "\n",
    "# Combine features and numeric data\n",
    "X_combined = hstack([tfidf_vectorizer_user_story.transform(df['Combined_Text']), scaler.transform(df[['Jumlah Scenario', 'Jumlah kata']])])\n",
    "\n",
    "# Perform cross-validation for Linear SVM\n",
    "svm_cv_scores = cross_val_score(svm_model, X_combined, df['Score'], cv=5)\n",
    "print(\"Cross-Validation Scores for Linear SVM:\", svm_cv_scores)\n",
    "print(\"Mean Accuracy: {:.2f}%\".format(svm_cv_scores.mean() * 100))\n",
    "\n",
    "# Perform cross-validation for AdaBoost with LinearSVC base learner\n",
    "adaboost_cv_scores = cross_val_score(adaboost_model, X_combined, df['Score'], cv=5)\n",
    "print(\"\\nCross-Validation Scores for AdaBoost with LinearSVC base learner:\", adaboost_cv_scores)\n",
    "print(\"Mean Accuracy: {:.2f}%\".format(adaboost_cv_scores.mean() * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "# Inisialisasi model Multinomial Naive Bayes\n",
    "nb_model = MultinomialNB()\n",
    "# Melatih model pada data latih\n",
    "nb_model.fit(X_train_combined, train_y)\n",
    "\n",
    "# Membuat prediksi pada data uji\n",
    "y_pred_nb = nb_model.predict(X_test_combined)\n",
    "\n",
    "# Evaluasi model Naive Bayes pada data uji\n",
    "accuracy_nb = accuracy_score(test_y, y_pred_nb)\n",
    "conf_matrix_nb = confusion_matrix(test_y, y_pred_nb)\n",
    "\n",
    "# Menampilkan hasil evaluasi\n",
    "print(\"Accuracy of Naive Bayes: {:.2f}%\".format(accuracy_nb * 100))\n",
    "print(\"\\nConfusion Matrix for Naive Bayes:\\n\", conf_matrix_nb)\n",
    "print(\"\\nClassification Report for Naive Bayes:\\n\", classification_report(test_y, y_pred_nb))\n",
    "\n",
    "# Plotting Confusion Matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix_nb, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Class 1\", \"Class 2\", \"Class 3\"], yticklabels=[\"Class 1\", \"Class 2\", \"Class 3\"])\n",
    "plt.title('Confusion Matrix for Multinomial Naive Bayes')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.show()\n",
    "\n",
    "# Inisialisasi model AdaBoost dengan Naive Bayes sebagai base estimator\n",
    "adaboost_model = AdaBoostClassifier(base_estimator=nb_model, n_estimators=50, random_state=42)\n",
    "\n",
    "# Melatih model pada data latih\n",
    "adaboost_model.fit(X_train_combined, train_y)\n",
    "\n",
    "# Membuat prediksi pada data uji\n",
    "y_pred_adaboost = adaboost_model.predict(X_test_combined)\n",
    "\n",
    "# Evaluasi model AdaBoost dengan Naive Bayes pada data uji\n",
    "accuracy_adaboost = accuracy_score(test_y, y_pred_adaboost)\n",
    "conf_matrix_adaboost = confusion_matrix(test_y, y_pred_adaboost)\n",
    "\n",
    "# Menampilkan hasil evaluasi\n",
    "print(\"Accuracy of AdaBoost with Naive Bayes: {:.2f}%\".format(accuracy_adaboost * 100))\n",
    "print(\"\\nConfusion Matrix for AdaBoost with Naive Bayes:\\n\", conf_matrix_adaboost)\n",
    "print(\"\\nClassification Report for AdaBoost with Naive Bayes:\\n\", classification_report(test_y, y_pred_adaboost))\n",
    "\n",
    "# Plotting Confusion Matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix_adaboost, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Class 1\", \"Class 2\", \"Class 3\"], yticklabels=[\"Class 1\", \"Class 2\", \"Class 3\"])\n",
    "plt.title('Confusion Matrix for AdaBoost with Multinomial Naive Bayes')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Inisialisasi model Multinomial Naive Bayes\n",
    "nb_model = MultinomialNB()\n",
    "\n",
    "# Melatih model pada data latih\n",
    "nb_model.fit(X_train_combined, train_y)\n",
    "\n",
    "# Membuat prediksi pada data uji\n",
    "y_pred_nb = nb_model.predict(X_test_combined)\n",
    "\n",
    "# Evaluasi model Naive Bayes pada data uji\n",
    "accuracy_nb = accuracy_score(test_y, y_pred_nb)\n",
    "conf_matrix_nb = confusion_matrix(test_y, y_pred_nb)\n",
    "\n",
    "# Menampilkan hasil evaluasi\n",
    "print(\"Accuracy of Naive Bayes: {:.2f}%\".format(accuracy_nb * 100))\n",
    "print(\"\\nConfusion Matrix for Naive Bayes:\\n\", conf_matrix_nb)\n",
    "print(\"\\nClassification Report for Naive Bayes:\\n\", classification_report(test_y, y_pred_nb))\n",
    "\n",
    "# Plotting Confusion Matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix_nb, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Class 1\", \"Class 2\", \"Class 3\"], yticklabels=[\"Class 1\", \"Class 2\", \"Class 3\"])\n",
    "plt.title('Confusion Matrix for Multinomial Naive Bayes')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.show()\n",
    "\n",
    "# Inisialisasi model AdaBoost dengan Naive Bayes sebagai base estimator\n",
    "adaboost_model = AdaBoostClassifier(base_estimator=nb_model, n_estimators=50, random_state=42)\n",
    "\n",
    "# Perform cross-validation for AdaBoost with Naive Bayes\n",
    "adaboost_cv_scores = cross_val_score(adaboost_model, X_train_combined, train_y, cv=5)\n",
    "print(\"\\nCross-Validation Scores for AdaBoost with Naive Bayes:\", adaboost_cv_scores)\n",
    "print(\"Mean Accuracy: {:.2f}%\".format(adaboost_cv_scores.mean() * 100))\n",
    "\n",
    "# Melatih model pada data latih (tanpa cross-validation)\n",
    "adaboost_model.fit(X_train_combined, train_y)\n",
    "\n",
    "# Membuat prediksi pada data uji\n",
    "y_pred_adaboost = adaboost_model.predict(X_test_combined)\n",
    "\n",
    "# Evaluasi model AdaBoost dengan Naive Bayes pada data uji\n",
    "accuracy_adaboost = accuracy_score(test_y, y_pred_adaboost)\n",
    "conf_matrix_adaboost = confusion_matrix(test_y, y_pred_adaboost)\n",
    "\n",
    "# Menampilkan hasil evaluasi\n",
    "print(\"\\nAccuracy of AdaBoost with Naive Bayes: {:.2f}%\".format(accuracy_adaboost * 100))\n",
    "print(\"\\nConfusion Matrix for AdaBoost with Naive Bayes:\\n\", conf_matrix_adaboost)\n",
    "print(\"\\nClassification Report for AdaBoost with Naive Bayes:\\n\", classification_report(test_y, y_pred_adaboost))\n",
    "\n",
    "# Plotting Confusion Matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix_adaboost, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Class 1\", \"Class 2\", \"Class 3\"], yticklabels=[\"Class 1\", \"Class 2\", \"Class 3\"])\n",
    "plt.title('Confusion Matrix for AdaBoost with Multinomial Naive Bayes')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coba tanpa konversi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from scipy.sparse import hstack\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Menggunakan MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "df['Combined_Text'] = df['Feature'] + ' ' + df['User Story'] + ' ' + df['Normal Flow'] + ' ' + df['Exception Flow'] + ' ' + df['Alternatif Flow']\n",
    "\n",
    "# Membagi dataset menjadi data latih dan data uji\n",
    "train, test = train_test_split(df, test_size=0.3)\n",
    "\n",
    "# Memilih kolom-kolom yang akan digunakan sebagai fitur dan label\n",
    "train_X = train[['Combined_Text']]\n",
    "train_numeric = scaler.fit_transform(train[['Jumlah Scenario', 'Jumlah kata']])\n",
    "train_y = train['Score']\n",
    "test_X = test[['Combined_Text']]\n",
    "test_y = test['Score']\n",
    "\n",
    "# Feature extraction (TF-IDF) for each column on training set\n",
    "tfidf_vectorizer_user_story = TfidfVectorizer(max_features=5000)\n",
    "X_train = tfidf_vectorizer_user_story.fit_transform(train_X['Combined_Text'])\n",
    "\n",
    "# Combine training set matrices\n",
    "X_train_combined = hstack([X_train, train_numeric])\n",
    "\n",
    "# Feature extraction (TF-IDF) for each column on test set\n",
    "X_test = tfidf_vectorizer_user_story.transform(test_X['Combined_Text'])\n",
    "\n",
    "# Combine test set matrices\n",
    "X_test_combined = hstack([X_test, scaler.transform(test[['Jumlah Scenario', 'Jumlah kata']])])\n",
    "\n",
    "# Model menggunakan LinearSVC\n",
    "svm_model = LinearSVC()  \n",
    "svm_model.fit(X_train_combined, train_y)\n",
    "\n",
    "# Evaluate svm_model on the test set\n",
    "y_pred_svm = svm_model.predict(X_test_combined)\n",
    "\n",
    "# Confusion Matrix for SVM\n",
    "conf_matrix_svm = confusion_matrix(test_y, y_pred_svm)\n",
    "\n",
    "# Plotting the heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix_svm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Class 1\", \"Class 2\", \"Class 3\"], yticklabels=[\"Class 1\", \"Class 2\", \"Class 3\"])\n",
    "plt.title('Confusion Matrix for Linear SVM')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Inisialisasi model Multinomial Naive Bayes\n",
    "nb_model = MultinomialNB()\n",
    "\n",
    "# Melatih model pada data latih\n",
    "nb_model.fit(X_train_combined, train_y)\n",
    "\n",
    "# Membuat prediksi pada data uji\n",
    "y_pred_nb = nb_model.predict(X_test_combined)\n",
    "\n",
    "# Evaluasi model Naive Bayes pada data uji\n",
    "accuracy_nb = accuracy_score(test_y, y_pred_nb)\n",
    "conf_matrix_nb = confusion_matrix(test_y, y_pred_nb)\n",
    "\n",
    "# Menampilkan hasil evaluasi\n",
    "print(\"Accuracy of Naive Bayes: {:.2f}%\".format(accuracy_nb * 100))\n",
    "print(\"\\nConfusion Matrix for Naive Bayes:\\n\", conf_matrix_nb)\n",
    "print(\"\\nClassification Report for Naive Bayes:\\n\", classification_report(test_y, y_pred_nb))\n",
    "\n",
    "# Plotting Confusion Matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix_nb, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Class 1\", \"Class 2\", \"Class 3\"], yticklabels=[\"Class 1\", \"Class 2\", \"Class 3\"])\n",
    "plt.title('Confusion Matrix for Multinomial Naive Bayes')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a= pd.DataFrame(X_train.todense().T,\n",
    "                index=tfidf_vectorizer_user_story.get_feature_names_out(),\n",
    "                columns=[f'D{i+1}' for i in range (len(train_X))])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_numeric.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a= pd.DataFrame(train_numeric_sparse.todense().T,\n",
    "                columns=[f'D{i+1}' for i in range (len(train_numeric))])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a= pd.DataFrame(X_train_combined.todense().T,\n",
    "                columns=[f'D{i+1}' for i in range (len(train_numeric))])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "from scipy.sparse import hstack\n",
    "import scipy\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "df['Combined_Text'] = df['Feature'] + ' ' + df['User Story'] + ' ' + df['Normal Flow'] + ' ' + df['Exception Flow'] + ' ' + df['Alternatif Flow']\n",
    "\n",
    "# Membagi dataset menjadi data latih dan data uji\n",
    "train, test = train_test_split(df, test_size=0.3)\n",
    "\n",
    "# Memilih kolom-kolom yang akan digunakan sebagai fitur dan label\n",
    "train_X = train[['Combined_Text']]\n",
    "train_numeric = scaler.fit_transform(train[['Jumlah Scenario']])\n",
    "train_y = train['Score']\n",
    "test_X = test[['Combined_Text']]\n",
    "test_y = test['Score']\n",
    "\n",
    "# Feature extraction (TF-IDF) for each column on training set\n",
    "tfidf_vectorizer_user_story = TfidfVectorizer(max_features=5000)\n",
    "X_train = tfidf_vectorizer_user_story.fit_transform(train_X['Combined_Text'])\n",
    "\n",
    "# Convert matriks numerik train set menjadi matriks sparse\n",
    "train_numeric_sparse = scipy.sparse.csr_matrix(train_numeric)\n",
    "\n",
    "# Combine training set matrices\n",
    "X_train_combined = hstack([X_train, train_numeric_sparse])\n",
    "\n",
    "# Feature extraction (TF-IDF) for each column on test set\n",
    "X_test = tfidf_vectorizer_user_story.transform(test_X['Combined_Text'])\n",
    "\n",
    "# Convert matriks numerik test set menjadi matriks sparse\n",
    "test_numeric_sparse = scipy.sparse.csr_matrix(scaler.transform(test[['Jumlah Scenario']]))\n",
    "\n",
    "# Combine test set matrices\n",
    "X_test_combined = hstack([X_test, test_numeric_sparse])\n",
    "\n",
    "\n",
    "svm_model = LinearSVC()  \n",
    "svm_model.fit(X_train_combined, train_y)\n",
    "\n",
    "# Evaluate svm_model on the test set\n",
    "y_pred_svm = svm_model.predict(X_test_combined)\n",
    "\n",
    "# Confusion Matrix for SVM\n",
    "conf_matrix_svm = confusion_matrix(test_y, y_pred_svm)\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Plotting the heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix_svm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Class 1\", \"Class 2\", \"Class 3\"], yticklabels=[\"Class 1\", \"Class 2\", \"Class 3\"])\n",
    "plt.title('Confusion Matrix for Linear SVM')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix_svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train the Logistic Regression model\n",
    "logistic_model = LogisticRegression()\n",
    "logistic_model.fit(X_train_combined, train_y)\n",
    "\n",
    "# Evaluate Logistic Regression on the test set\n",
    "y_pred_logistic = logistic_model.predict(X_test_combined)\n",
    "\n",
    "# Confusion Matrix for Logistic Regression\n",
    "conf_matrix_logistic = confusion_matrix(test_y, y_pred_logistic)\n",
    "print(\"Confusion Matrix for Logistic Regression:\")\n",
    "print(conf_matrix_logistic)\n",
    "\n",
    "# Additional evaluation metrics\n",
    "accuracy_logistic = accuracy_score(test_y, y_pred_logistic)\n",
    "print(f\"\\nAccuracy for Logistic Regression: {accuracy_logistic:.2f}\")\n",
    "\n",
    "classification_report_logistic = classification_report(test_y, y_pred_logistic)\n",
    "print(\"\\nClassification Report for Logistic Regression:\")\n",
    "print(classification_report_logistic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "rf_model = RandomForestClassifier()\n",
    "rf_model.fit(X_train_combined, train_y)\n",
    "\n",
    "# Evaluate Random Forest on the test set\n",
    "y_pred_rf = rf_model.predict(X_test_combined)\n",
    "\n",
    "# Confusion Matrix for Random Forest\n",
    "conf_matrix_rf = confusion_matrix(test_y, y_pred_rf)\n",
    "print(\"\\nConfusion Matrix for Random Forest:\")\n",
    "print(conf_matrix_rf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stacking classifier without final_estimator\n",
    "stacking_model = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('svm', svm_model),\n",
    "        ('lr', logistic_model),\n",
    "        ('rf', rf_model)\n",
    "    ],\n",
    "    cv=2  # Adjust the number of splits as needed\n",
    ")\n",
    "\n",
    "# Train the stacking model\n",
    "stacking_model.fit(X_train_combined, train_y)\n",
    "\n",
    "# Evaluate stacking model on the test set\n",
    "y_pred_stacking = stacking_model.predict(X_test_combined)\n",
    "\n",
    "# Confusion Matrix for Stacking\n",
    "conf_matrix_stacking = confusion_matrix(test_y, y_pred_stacking)\n",
    "print(\"Confusion Matrix for Stacking:\")\n",
    "print(conf_matrix_stacking)\n",
    "\n",
    "# Additional evaluation metrics\n",
    "accuracy_stacking = accuracy_score(test_y, y_pred_stacking)\n",
    "print(f\"\\nAccuracy for Stacking: {accuracy_stacking:.2f}\")\n",
    "\n",
    "classification_report_stacking = classification_report(test_y, y_pred_stacking)\n",
    "print(\"\\nClassification Report for Stacking:\")\n",
    "print(classification_report_stacking)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Menampilkan matriks sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a= pd.DataFrame(X_train.todense().T,\n",
    "                index=tfidf_vectorizer_user_story.get_feature_names_out(),\n",
    "                columns=[f'D{i+1}' for i in range (len(train_X))])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a= pd.DataFrame(train_numeric_sparse.todense().T,\n",
    "                columns=[f'D{i+1}' for i in range (len(train_numeric))])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a= pd.DataFrame(X_train_combined.todense().T,\n",
    "                columns=[f'D{i+1}' for i in range (len(train_numeric))])\n",
    "a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
