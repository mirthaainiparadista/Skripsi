{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bismillah Bisa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Nama Fitur</th>\n",
       "      <th>User Story</th>\n",
       "      <th>Scenario</th>\n",
       "      <th>Scenario.1</th>\n",
       "      <th>Scenario.2</th>\n",
       "      <th>Scenario.3</th>\n",
       "      <th>Scenario.4</th>\n",
       "      <th>Scenario.5</th>\n",
       "      <th>Scenario.6</th>\n",
       "      <th>Scenario.7</th>\n",
       "      <th>Scenario.8</th>\n",
       "      <th>Scenario.9</th>\n",
       "      <th>Scenario.10</th>\n",
       "      <th>Effort</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Create Program Outcomes</td>\n",
       "      <td>In order to establish Program Outcomes\\nAs Cur...</td>\n",
       "      <td>Scenario: Create Program Outcomes Successfully...</td>\n",
       "      <td>Scenario: Failed to create Program Outcomes as...</td>\n",
       "      <td>Scenario: Failed to Create Program Outcomes Du...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Read Program Outcomes</td>\n",
       "      <td>In order to evaluate Program Outcomes\\nAs Curr...</td>\n",
       "      <td>Scenario: Program Outcomes Provided\\nGiven I a...</td>\n",
       "      <td>Scenario: Empty Program Outcomes\\nGiven I am o...</td>\n",
       "      <td>Scenario: Program Outcomes Export to Excel\\nGi...</td>\n",
       "      <td>Scenario: Program Outcomes Export to PDF\\nGive...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Update Program Outcomes</td>\n",
       "      <td>In order to improve Program Outcomes\\nAs Curri...</td>\n",
       "      <td>Scenario: Update Program Outcomes Successfully...</td>\n",
       "      <td>Scenario: Failed to update Program Outcomes as...</td>\n",
       "      <td>Scenario: Failed to update Program Outcomes Du...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Delete Program Outcomes</td>\n",
       "      <td>In order to remove irrelevant Program Outcomes...</td>\n",
       "      <td>Scenario: Delete Program Outcomes Successfully...</td>\n",
       "      <td>Scenario: Failed to delete Program Outcomes Du...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Create SNDikti Learning Outcomes</td>\n",
       "      <td>In order to adapt SNDikti Learning Outcomes\\nA...</td>\n",
       "      <td>Scenario: Create SNDikti Learning Outcome Succ...</td>\n",
       "      <td>Scenario: Failed to create SNDikti Learning Ou...</td>\n",
       "      <td>Scenario: Failed to Create SNDikti Learning Ou...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>Read Study Program</td>\n",
       "      <td>In order to verify Study Program Data\\nAs Curr...</td>\n",
       "      <td>Scenario: Study Program Provided\\nGiven I am o...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>Update Study Program</td>\n",
       "      <td>In order to improve Study Program Data \\nAs Cu...</td>\n",
       "      <td>Scenario: Update Study Program Successfully\\nG...</td>\n",
       "      <td>Scenario: Failed to update Study Program Due t...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>Read User Management</td>\n",
       "      <td>In order to verify User\\nAs Curriculum Team,\\n...</td>\n",
       "      <td>Scenario: User Provided\\nGiven I am on “http:/...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>Update User Management</td>\n",
       "      <td>In order to improve User Data \\nAs Curriculum ...</td>\n",
       "      <td>Scenario: Update User Successfully\\nGiven I am...</td>\n",
       "      <td>Scenario: Failed to update User Due to Empty D...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>Delete User Management</td>\n",
       "      <td>In order to remove irrelevant User\\nAs Curricu...</td>\n",
       "      <td>Scenario: Delete User Management Successfully\\...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>73 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Nama Fitur  \\\n",
       "0            Create Program Outcomes   \n",
       "1              Read Program Outcomes   \n",
       "2            Update Program Outcomes   \n",
       "3            Delete Program Outcomes   \n",
       "4   Create SNDikti Learning Outcomes   \n",
       "..                               ...   \n",
       "68                Read Study Program   \n",
       "69              Update Study Program   \n",
       "70              Read User Management   \n",
       "71            Update User Management   \n",
       "72            Delete User Management   \n",
       "\n",
       "                                           User Story  \\\n",
       "0   In order to establish Program Outcomes\\nAs Cur...   \n",
       "1   In order to evaluate Program Outcomes\\nAs Curr...   \n",
       "2   In order to improve Program Outcomes\\nAs Curri...   \n",
       "3   In order to remove irrelevant Program Outcomes...   \n",
       "4   In order to adapt SNDikti Learning Outcomes\\nA...   \n",
       "..                                                ...   \n",
       "68  In order to verify Study Program Data\\nAs Curr...   \n",
       "69  In order to improve Study Program Data \\nAs Cu...   \n",
       "70  In order to verify User\\nAs Curriculum Team,\\n...   \n",
       "71  In order to improve User Data \\nAs Curriculum ...   \n",
       "72  In order to remove irrelevant User\\nAs Curricu...   \n",
       "\n",
       "                                             Scenario  \\\n",
       "0   Scenario: Create Program Outcomes Successfully...   \n",
       "1   Scenario: Program Outcomes Provided\\nGiven I a...   \n",
       "2   Scenario: Update Program Outcomes Successfully...   \n",
       "3   Scenario: Delete Program Outcomes Successfully...   \n",
       "4   Scenario: Create SNDikti Learning Outcome Succ...   \n",
       "..                                                ...   \n",
       "68  Scenario: Study Program Provided\\nGiven I am o...   \n",
       "69  Scenario: Update Study Program Successfully\\nG...   \n",
       "70  Scenario: User Provided\\nGiven I am on “http:/...   \n",
       "71  Scenario: Update User Successfully\\nGiven I am...   \n",
       "72  Scenario: Delete User Management Successfully\\...   \n",
       "\n",
       "                                           Scenario.1  \\\n",
       "0   Scenario: Failed to create Program Outcomes as...   \n",
       "1   Scenario: Empty Program Outcomes\\nGiven I am o...   \n",
       "2   Scenario: Failed to update Program Outcomes as...   \n",
       "3   Scenario: Failed to delete Program Outcomes Du...   \n",
       "4   Scenario: Failed to create SNDikti Learning Ou...   \n",
       "..                                                ...   \n",
       "68                                                      \n",
       "69  Scenario: Failed to update Study Program Due t...   \n",
       "70                                                      \n",
       "71  Scenario: Failed to update User Due to Empty D...   \n",
       "72                                                      \n",
       "\n",
       "                                           Scenario.2  \\\n",
       "0   Scenario: Failed to Create Program Outcomes Du...   \n",
       "1   Scenario: Program Outcomes Export to Excel\\nGi...   \n",
       "2   Scenario: Failed to update Program Outcomes Du...   \n",
       "3                                                       \n",
       "4   Scenario: Failed to Create SNDikti Learning Ou...   \n",
       "..                                                ...   \n",
       "68                                                      \n",
       "69                                                      \n",
       "70                                                      \n",
       "71                                                      \n",
       "72                                                      \n",
       "\n",
       "                                           Scenario.3 Scenario.4 Scenario.5  \\\n",
       "0                                                                             \n",
       "1   Scenario: Program Outcomes Export to PDF\\nGive...                         \n",
       "2                                                                             \n",
       "3                                                                             \n",
       "4                                                                             \n",
       "..                                                ...        ...        ...   \n",
       "68                                                                            \n",
       "69                                                                            \n",
       "70                                                                            \n",
       "71                                                                            \n",
       "72                                                                            \n",
       "\n",
       "   Scenario.6 Scenario.7 Scenario.8 Scenario.9 Scenario.10 Effort  \n",
       "0                                                               M  \n",
       "1                                                               M  \n",
       "2                                                               M  \n",
       "3                                                               S  \n",
       "4                                                               M  \n",
       "..        ...        ...        ...        ...         ...    ...  \n",
       "68                                                              S  \n",
       "69                                                              M  \n",
       "70                                                              M  \n",
       "71                                                              M  \n",
       "72                                                              S  \n",
       "\n",
       "[73 rows x 14 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_excel(\"C:/Users/Mirtha/Downloads/Source Code Skripsi/Data Labelling Sendiri.xlsx\")\n",
    "df=df.fillna('')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Menggabungkan data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['combined'] = df.iloc[:, :13].apply(lambda x: '\\n'.join(x), axis=1)\n",
    "\n",
    "# Menampilkan hasil\n",
    "print(df.loc[0, 'combined'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Menghitung Jumlah Skenario dan Kata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['combined'] = df['combined'].str.lower()\n",
    "df['number_word'] = df['combined'].apply(lambda x: len(x.split()))\n",
    "df['number_scenario'] = df['combined'].apply(lambda x: x.count(\"scenario:\"))\n",
    "df['number_word']\n",
    "df['number_scenario'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalisasi Fitur Numerik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# Inisialisasi MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "# Normalisasi 'number_word' dan 'number_scenario' dengan MinMaxScaler\n",
    "df[['number_word', 'number_scenario']] = scaler.fit_transform(df[['number_word', 'number_scenario']])\n",
    "df[['number_word', 'number_scenario']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "# Function to remove punctuation\n",
    "def remove_punctuation(text):\n",
    "    punctuation_to_replace = string.punctuation + \"‘’“”\"\n",
    "    translation_table = str.maketrans(punctuation_to_replace, ' ' * len(punctuation_to_replace))\n",
    "    cleaned_text = text.translate(translation_table)\n",
    "    return cleaned_text\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "df['combined'] = df['combined'].apply(remove_punctuation)\n",
    "print(df.loc[0, 'combined'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Load English stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Lemmatizer initialization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Tokenize the text in the 'combined' column\n",
    "df['combined'] = df['combined'].apply(lambda x: word_tokenize(x))\n",
    "print(df['combined'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stopwords Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove stopwords\n",
    "def remove_stopwords(tokens):\n",
    "    return [token for token in tokens if token.lower() not in stop_words]\n",
    "\n",
    "# Remove stopwords from the tokenized text\n",
    "df['combined'] = df['combined'].apply(remove_stopwords)\n",
    "df['combined']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lematisasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define stopwords and lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to perform lemmatization\n",
    "def lemmatize_tokens(tokens):\n",
    "    return [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "df['combined'] = df['combined'].apply(lemmatize_tokens)\n",
    "df['combined']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Convert lemmatized tokens back to strings\n",
    "df['combined'] = df['combined'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# Initialize TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the lemmatized text\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df['combined'])\n",
    "\n",
    "# Convert the TF-IDF matrix to a DataFrame\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "tfidf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate 'effort' column of the original DataFrame with TF-IDF DataFrame\n",
    "integrated_data = pd.concat([tfidf_df, df[['number_word', 'number_scenario', 'Effort']]], axis=1)\n",
    "integrated_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One hot encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# One-Hot Encoding for 'Effort'\n",
    "onehot_encoder = OneHotEncoder()\n",
    "effort_encoded = onehot_encoder.fit_transform(integrated_data['Effort'].values.reshape(-1,1))\n",
    "effort_encoded_array = effort_encoded.toarray()\n",
    "\n",
    "# Mengambil indeks kolom dengan nilai maksimum untuk setiap baris\n",
    "effort_labels = np.argmax(effort_encoded_array, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Melihat kategori yang diencode\n",
    "encoded_categories = onehot_encoder.categories_\n",
    "print(\"Encoded Categories:\", encoded_categories)\n",
    "\n",
    "# Mencetak nilai yang sesuai untuk setiap kategori\n",
    "for i, category in enumerate(encoded_categories[0]):\n",
    "    print(f\"Kategori '{category}' diencode menjadi:\", i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data into features (X) and target variable (y)\n",
    "X = integrated_data.drop(columns=['Effort'])\n",
    "y = effort_encoded_array\n",
    "\n",
    "# Splitting data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data into features (X) and target variable (y)\n",
    "X = integrated_data.drop(columns=['Effort'])\n",
    "y = effort_labels\n",
    "\n",
    "# Define AdaBoost Classifier\n",
    "adaboost_model = AdaBoostClassifier()\n",
    "\n",
    "# Perform k-fold cross-validation\n",
    "cv_scores = cross_val_score(adaboost_model, X, y, cv=5)  # cv=5 for 5-fold cross-validation\n",
    "\n",
    "# Print the cross-validation scores\n",
    "print(\"Cross-validation scores:\", cv_scores)\n",
    "print(\"Mean cross-validation score:\", cv_scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adaboost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdaBoost Classifier\n",
    "adaboost_model = AdaBoostClassifier()\n",
    "adaboost_model.fit(X_train, y_train_encoded)\n",
    "adaboost_pred = adaboost_model.predict(X_test)\n",
    "adaboost_accuracy = accuracy_score(y_test_encoded, adaboost_pred)\n",
    "print(\"AdaBoost Accuracy:\", adaboost_accuracy)\n",
    "print(\"AdaBoost Classification Report:\\n\", classification_report(y_test_encoded, adaboost_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Membuat daftar lengkap kelas\n",
    "full_class_list = np.unique(np.concatenate((y_train_encoded, y_test_encoded)))\n",
    "\n",
    "# Menghitung confusion matrix dengan daftar lengkap kelas\n",
    "conf_matrix_adaboost = confusion_matrix(y_test_encoded, adaboost_pred, labels=full_class_list)\n",
    "\n",
    "# Visualisasi confusion matrix sebagai gambar\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix_adaboost, annot=True, fmt=\"d\", cmap=\"Blues\", \n",
    "            xticklabels=full_class_list,\n",
    "            yticklabels=full_class_list)\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix for AdaBoost Classifier')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree Classifier\n",
    "decision_tree_model = DecisionTreeClassifier()\n",
    "decision_tree_model.fit(X_train, y_train_encoded)\n",
    "decision_tree_pred = decision_tree_model.predict(X_test)\n",
    "decision_tree_accuracy = accuracy_score(y_test_encoded, decision_tree_pred)\n",
    "print(\"Decision Tree Accuracy:\", decision_tree_accuracy)\n",
    "print(\"Decision Tree Classification Report:\\n\", classification_report(y_test_encoded, decision_tree_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "\n",
    "# Visualisasi decision tree\n",
    "plt.figure(figsize=(20,10))\n",
    "plot_tree(decision_tree_model, filled=True, feature_names=X_train.columns, class_names=np.unique(y_train_encoded).astype(str))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menghitung confusion matrix dengan daftar lengkap kelas\n",
    "conf_matrix_decision_tree = confusion_matrix(y_test_encoded, decision_tree_pred, labels=full_class_list)\n",
    "\n",
    "# Visualisasi confusion matrix sebagai gambar\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix_decision_tree, annot=True, fmt=\"d\", cmap=\"Blues\", \n",
    "            xticklabels=full_class_list,\n",
    "            yticklabels=full_class_list)\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix for Decision Tree Classifier')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multinomial Naive Bayes Classifier\n",
    "naive_bayes_model = MultinomialNB()\n",
    "naive_bayes_model.fit(X_train, y_train_encoded)\n",
    "naive_bayes_pred = naive_bayes_model.predict(X_test)\n",
    "naive_bayes_accuracy = accuracy_score(y_test_encoded, naive_bayes_pred)\n",
    "print(\"Multinomial Naive Bayes Accuracy:\", naive_bayes_accuracy)\n",
    "print(\"Multinomial Naive Bayes Classification Report:\\n\", classification_report(y_test_encoded, naive_bayes_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menghitung confusion matrix dengan daftar lengkap kelas\n",
    "conf_matrix_naive_bayes = confusion_matrix(y_test_encoded, naive_bayes_pred, labels=full_class_list)\n",
    "\n",
    "# Visualisasi confusion matrix sebagai gambar\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix_naive_bayes, annot=True, fmt=\"d\", cmap=\"Blues\", \n",
    "            xticklabels=full_class_list,\n",
    "            yticklabels=full_class_list)\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix for Multinomial Naive Bayes Classifier')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediksi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contoh paragraf yang ingin diprediksi\n",
    "paragraph = \"\"\"Create Program Outcomes\t\"In order to establish Program Outcomes\n",
    "As Curriculum Team,\n",
    "I want the capability to add Program Outcomes\"\t\"Scenario: Create Program Outcomes Successfully\n",
    "Given I am on “http://127.0.0.1:8000/dashboard/curriculum”\n",
    "When I press “Data”\n",
    "And I press “Program Outcomes”\n",
    "Then I should be on “Program Outcomes Page”\n",
    "When I press “Add”\n",
    "Then I should be on “Program Outcomes Creation Page”\n",
    "When I fill in “Program Outcome Code” with “PO1”\n",
    "And I fill in “Program Outcome Description” with “Graduates possess the ability to analyze, design, create, and comprehensively evaluate information systems in alignment with organizational goals, demonstrating effective proficiency.”\n",
    "And I press “Add Program Outcome”\n",
    "Then the response should contain “Success! Program Outcome has been added.”\"\t\"Scenario: Failed to create Program Outcomes as the Program Outcome Code has already been taken.\n",
    "Given I am on “http://127.0.0.1:8000/dashboard/curriculum”\n",
    "When I press “Data”\n",
    "And I press “Program Outcomes”\n",
    "Then I should be on “Program Outcomes Page”\n",
    "And the “Program Outcome Code” field should contain “PO1”\n",
    "When I press “Add”\n",
    "Then I should be on “Program Outcomes Creation Page”\n",
    "When I fill in “Program Outcome Code” with “PO1”\n",
    "And I fill in “Program Outcome Description” with “Graduates possess the ability to analyze, design, create, and comprehensively evaluate information systems in alignment with organizational goals, demonstrating effective proficiency.”\n",
    "And I press “Add Program Outcome”\n",
    "Then the response should contain “The Program Outcome Code has already been taken.”\"\t\"Scenario: Failed to Create Program Outcomes Due to Empty Data Fields\n",
    "Given I am on “http://127.0.0.1:8000/dashboard/curriculum”\n",
    "When I press “Data”\n",
    "And I press “Program Outcomes”\n",
    "Then I should be on “Program Outcomes Page”\n",
    "And the “Program Outcome Code” field should contain “PO1”\n",
    "When I press “Add”\n",
    "Then I should be on “Program Outcomes Creation Page”\n",
    "When I fill in “Program Outcome Code” with “PO2”\n",
    "And I press “Add Program Outcome”\n",
    "Then the response should contain “Program Outcome Description field is required”.\"\"\"\n",
    "\"\n",
    "\n",
    "# Pra-pemrosesan teks (misalnya tokenisasi, menghapus stopwords, normalisasi)\n",
    "# ...\n",
    "\n",
    "# Vektorisasi paragraf\n",
    "paragraph_vectorized = vectorizer.transform([paragraph])  # Menggunakan vektorisasi yang sama yang digunakan pada data latih\n",
    "\n",
    "# Prediksi dengan model AdaBoost\n",
    "predicted_label = adaboost_model.predict(paragraph_vectorized)\n",
    "\n",
    "# Keluarkan hasil prediksi\n",
    "print(\"Hasil Prediksi untuk Paragraf:\")\n",
    "print(predicted_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data into features (X) and target variable (y)\n",
    "X = integrated_data.drop(columns=['Effort'])\n",
    "y = integrated_data['Effort']\n",
    "\n",
    "# Splitting data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# Perform one-hot encoding on the target variable\n",
    "onehot_encoder = OneHotEncoder()\n",
    "y_train_encoded = onehot_encoder.fit_transform(y_train.values.reshape(-1,1)).toarray()  # Convert to dense array\n",
    "y_test_encoded = onehot_encoder.transform(y_test.values.reshape(-1,1)).toarray()  # Convert to dense array\n",
    "\n",
    "# Train AdaBoost model\n",
    "adaboost_model = AdaBoostClassifier()\n",
    "adaboost_model.fit(X_train, y_train_encoded)\n",
    "\n",
    "# Predict\n",
    "y_pred = adaboost_model.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "accuracy = accuracy_score(y_test_encoded, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\\n\", classification_report(y_test_encoded, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# One-Hot Encoding for 'Effort'\n",
    "onehot_encoder = OneHotEncoder()\n",
    "effort_encoded = onehot_encoder.fit_transform(integrated_data['Effort'].values.reshape(-1,1))\n",
    "\n",
    "# Splitting data into features (X) and target variable (y)\n",
    "X = integrated_data.drop(columns=['Effort'])\n",
    "y = effort_encoded\n",
    "\n",
    "# Splitting data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Model Creation and Evaluation\n",
    "\n",
    "# AdaBoost Classifier\n",
    "adaboost_model = AdaBoostClassifier()\n",
    "adaboost_model.fit(X_train, y_train)\n",
    "adaboost_pred = adaboost_model.predict(X_test)\n",
    "adaboost_accuracy = accuracy_score(y_test, adaboost_pred)\n",
    "print(\"AdaBoost Accuracy:\", adaboost_accuracy)\n",
    "print(\"AdaBoost Classification Report:\\n\", classification_report(y_test, adaboost_pred))\n",
    "\n",
    "# Decision Tree Classifier\n",
    "decision_tree_model = DecisionTreeClassifier()\n",
    "decision_tree_model.fit(X_train, y_train)\n",
    "decision_tree_pred = decision_tree_model.predict(X_test)\n",
    "decision_tree_accuracy = accuracy_score(y_test, decision_tree_pred)\n",
    "print(\"Decision Tree Accuracy:\", decision_tree_accuracy)\n",
    "print(\"Decision Tree Classification Report:\\n\", classification_report(y_test, decision_tree_pred))\n",
    "\n",
    "# Multinomial Naive Bayes Classifier\n",
    "naive_bayes_model = MultinomialNB()\n",
    "naive_bayes_model.fit(X_train, y_train)\n",
    "naive_bayes_pred = naive_bayes_model.predict(X_test)\n",
    "naive_bayes_accuracy = accuracy_score(y_test, naive_bayes_pred)\n",
    "print(\"Multinomial Naive Bayes Accuracy:\", naive_bayes_accuracy)\n",
    "print(\"Multinomial Naive Bayes Classification Report:\\n\", classification_report(y_test, naive_bayes_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Case Folding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Feature']=df['Feature'].str.lower()\n",
    "df['User Story']=df['User Story'].str.lower()\n",
    "df['Normal Flow']=df['Normal Flow'].str.lower()\n",
    "df['Exception Flow']=df['Exception Flow'].str.lower()\n",
    "df['Alternatif Flow']=df['Alternatif Flow'].str.lower()\n",
    "label_mapping = {'Mudah': 0, 'Cukup': 1, 'Sulit': 2}\n",
    "df['Label_Numeric'] = df['Score'].map(label_mapping)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove URL, html, dan punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def remove_URL(text):\n",
    "    def replace_url(match):\n",
    "        return \"\"\n",
    "\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url_pattern.sub(replace_url, text)\n",
    "\n",
    "# ternyata tidak ada sepertinya\n",
    "def remove_html(text): \n",
    "    def replace_html(match):\n",
    "        return \"\"\n",
    "    html=re.compile(r'<.*?>')\n",
    "    return html.sub(replace_html,text)\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    table=str.maketrans('','',string.punctuation)\n",
    "    return text.translate(table)\n",
    "\n",
    "selected_columns = ['Feature', 'User Story', 'Normal Flow', 'Exception Flow', 'Alternatif Flow']\n",
    "\n",
    "for col in selected_columns:\n",
    "    for i in range(len(df[col])):\n",
    "        df.at[i, col] = remove_URL(df.at[i, col])\n",
    "        df.at[i, col] = remove_html(df.at[i, col])\n",
    "        df.at[i, col] = remove_punctuation(df.at[i, col])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spelling Correction (JANGAN DULU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spellchecker import SpellChecker\n",
    "\n",
    "spell = SpellChecker()\n",
    "\n",
    "def correct_spellings(text):\n",
    "    corrected_text = []\n",
    "    misspelled_words = spell.unknown(text.split())\n",
    "    for word in text.split():\n",
    "        if word in misspelled_words:\n",
    "            corrected_word = spell.correction(word)\n",
    "            if corrected_word is not None:\n",
    "                corrected_text.append(corrected_word)\n",
    "            else:\n",
    "                # Handle the case where correction is None\n",
    "                corrected_text.append(word)\n",
    "        else:\n",
    "            corrected_text.append(word)\n",
    "    return \" \".join(corrected_text)\n",
    "\n",
    "for col in selected_columns:\n",
    "    for i in range(len(df[col])):\n",
    "        df.at[i, col] = correct_spellings(df.at[i, col])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"corect me plese\"\n",
    "correct_spellings(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "selected_columns = ['Feature', 'User Story', 'Normal Flow', 'Exception Flow', 'Alternatif Flow']\n",
    "\n",
    "for col in selected_columns:\n",
    "    for i in range(len(df[col])):\n",
    "        df.at[i, col] = nltk.word_tokenize(df.at[i, col])\n",
    "\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in selected_columns:\n",
    "    for i in range(len(df[col])):\n",
    "        first_text =  df.at[i, col]\n",
    "        first_text_list_cleaned = [word for word in first_text if word.lower() not in stopwords]\n",
    "        df.at[i, col] = first_text_list_cleaned\n",
    "df\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemm = WordNetLemmatizer()\n",
    "\n",
    "for col in selected_columns:\n",
    "    for i in range(len(df[col])):\n",
    "        # Tokenize each text\n",
    "        tokens = df.at[i, col]\n",
    "        # Lemmatize each word\n",
    "        lemmatized_tokens = [lemm.lemmatize(word) for word in tokens]\n",
    "        # Join the lemmatized tokens back into a string\n",
    "        lemmatized_text = ' '.join(lemmatized_tokens)\n",
    "        # Update the DataFrame with the lemmatized text\n",
    "        df.at[i, col] = lemmatized_text\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualisasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "from io import BytesIO\n",
    "from PIL import Image  # Tambahkan baris ini\n",
    "import numpy as np\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "url = \"https://user-images.githubusercontent.com/74188336/142692890-641ebc21-2e47-4556-9d37-1c0b9e1a0587.jpeg\"\n",
    "response = requests.get(url)\n",
    "img = Image.open(BytesIO(response.content))\n",
    "\n",
    "text = ' '.join(df['Feature'].values)\n",
    "mask = np.array(img)\n",
    "wordcloud = WordCloud(max_font_size=50, max_words=1000, background_color=\"white\", mask=mask, colormap='BuGn').generate(text.lower())\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://user-images.githubusercontent.com/74188336/142692894-c17240e4-1101-4591-9d10-71793e460816.jpeg\"\n",
    "response = requests.get(url)\n",
    "img = Image.open(BytesIO(response.content))\n",
    "\n",
    "text = ' '.join(df['User Story'].values)\n",
    "mask = np.array(img)\n",
    "wordcloud = WordCloud(max_font_size=50, max_words=1000, background_color=\"white\", mask=mask, colormap='BuGn').generate(text.lower())\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Create and train the Multinomial Logistic Regression model\n",
    "multinomial_model = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
    "# Gunakan cross_val_predict untuk mendapatkan label prediksi untuk setiap lipatan cross-validation\n",
    "y_pred_multinomial = cross_val_predict(multinomial_model, X_combined, y, cv=stratified_kfold)\n",
    "\n",
    "# Hitung confusion matrix\n",
    "conf_matrix_multinomial = confusion_matrix(y, y_pred_multinomial)\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix_multinomial)\n",
    "\n",
    "# Visualisasikan confusion matrix jika diperlukan\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix_multinomial, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Class 1\", \"Class 2\", \"Class 3\"], yticklabels=[\"Class 1\", \"Class 2\", \"Class 3\"])\n",
    "plt.title('Confusion Matrix (Cross-Validation)')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.show()\n",
    "\n",
    "# Additional evaluation metrics\n",
    "accuracy_multinomial = accuracy_score(y, y_pred_multinomial)\n",
    "print(f\"\\nAccuracy for Multinomial Logistic Regression: {accuracy_multinomial:.2f}\")\n",
    "\n",
    "classification_report_multinomial = classification_report(y, y_pred_multinomial)\n",
    "print(\"\\nClassification Report for Multinomial Logistic Regression:\")\n",
    "print(classification_report_multinomial)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "rf_model = RandomForestClassifier()\n",
    "\n",
    "# Gunakan cross_val_predict dengan StratifiedKFold\n",
    "y_pred_rf = cross_val_predict(rf_model, X_combined, y, cv=stratified_kfold)\n",
    "\n",
    "# Hitung confusion matrix\n",
    "conf_matrix_rf = confusion_matrix(y, y_pred_rf)\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix_rf)\n",
    "\n",
    "# Visualisasikan confusion matrix jika diperlukan\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix_rf, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Class 1\", \"Class 2\", \"Class 3\"], yticklabels=[\"Class 1\", \"Class 2\", \"Class 3\"])\n",
    "plt.title('Confusion Matrix (Cross-Validation)')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.show()\n",
    "\n",
    "# Additional evaluation metrics\n",
    "accuracy_rf = accuracy_score(y, y_pred_rf)\n",
    "print(f\"\\nAccuracy for Random Forest: {accuracy_rf:.2f}\")\n",
    "\n",
    "classification_report_rf = classification_report(y, y_pred_rf)\n",
    "print(\"\\nClassification Report for Random Forest:\")\n",
    "print(classification_report_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.model_selection import cross_val_predict, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from scipy.sparse import hstack\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Specify the number of folds (e.g., 5 or 10)\n",
    "num_folds = 5\n",
    "\n",
    "# Create a StratifiedKFold object with the desired number of splits\n",
    "stratified_kfold = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Menggabungkan semua fitur ke dalam matriks tunggal\n",
    "df['Combined_Text'] = df['Feature'] + ' ' + df['User Story'] + ' ' + df['Normal Flow'] + ' ' + df['Exception Flow'] + ' ' + df['Alternatif Flow']\n",
    "X_combined = TfidfVectorizer(max_features=5000).fit_transform(df['Combined_Text'])\n",
    "X_combined = hstack([X_combined, df[['Jumlah Scenario', 'Jumlah kata']]])\n",
    "\n",
    "# Memilih kolom-kolom yang akan digunakan sebagai label\n",
    "y = df['Score']\n",
    "\n",
    "# Inisialisasi model Linear SVM, Logistic Regression, dan Random Forest\n",
    "svm_model = LinearSVC()\n",
    "logistic_model = LogisticRegression()\n",
    "rf_model = RandomForestClassifier()\n",
    "\n",
    "# Membuat Stacking Ensemble dengan model-model yang telah diinisialisasi\n",
    "stacking_model = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('svm', svm_model),\n",
    "        ('logistic', logistic_model),\n",
    "        ('random_forest', rf_model)\n",
    "    ],\n",
    "    cv=stratified_kfold\n",
    ")\n",
    "\n",
    "\n",
    "# Gunakan cross_val_predict untuk mendapatkan label prediksi untuk setiap lipatan cross-validation\n",
    "y_pred_stacking = cross_val_predict(stacking_model, X_combined, y.values.ravel(), cv=stratified_kfold)\n",
    "\n",
    "\n",
    "# Hitung confusion matrix\n",
    "conf_matrix_stacking = confusion_matrix(y, y_pred_stacking)\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix_stacking)\n",
    "\n",
    "# Visualisasikan confusion matrix jika diperlukan\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix_stacking, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Class 1\", \"Class 2\", \"Class 3\"], yticklabels=[\"Class 1\", \"Class 2\", \"Class 3\"])\n",
    "plt.title('Confusion Matrix (Stacking Ensemble)')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.show()\n",
    "\n",
    "# Additional evaluation metrics\n",
    "accuracy_stacking = accuracy_score(y, y_pred_stacking)\n",
    "print(f\"\\nAccuracy for Stacking Ensemble: {accuracy_stacking:.2f}\")\n",
    "\n",
    "classification_report_stacking = classification_report(y, y_pred_stacking)\n",
    "print(\"\\nClassification Report for Stacking Ensemble:\")\n",
    "print(classification_report_stacking)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "# Stacking classifier without final_estimator\n",
    "stacking_model = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('svm', svm_model),\n",
    "        ('lr', logistic_model),\n",
    "        ('rf', rf_model)\n",
    "    ],\n",
    ")\n",
    "\n",
    "y_pred_stacking = cross_val_predict(stacking_model, X_combined, y.values.ravel(), cv=stratified_kfold)\n",
    "\n",
    "# Hitung confusion matrix\n",
    "conf_matrix_stacking = confusion_matrix(y, y_pred_stacking)\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix_stacking)\n",
    "\n",
    "# Visualisasikan confusion matrix jika diperlukan\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix_stacking, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Class 1\", \"Class 2\", \"Class 3\"], yticklabels=[\"Class 1\", \"Class 2\", \"Class 3\"])\n",
    "plt.title('Confusion Matrix (Cross-Validation)')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.show()\n",
    "\n",
    "# Additional evaluation metrics\n",
    "accuracy_stacking = accuracy_score(y, y_pred_stacking)\n",
    "print(f\"\\nAccuracy for Ensemble Stacking Model: {accuracy_stacking:.2f}\")\n",
    "\n",
    "classification_report_stacking = classification_report(y, y_pred_stacking)\n",
    "print(\"\\nClassification Report for Ensemble Stacking Model:\")\n",
    "print(classification_report_stacking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from scipy.sparse import hstack\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Menggunakan MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "df['Combined_Text'] = df['Feature'] + ' ' + df['User Story'] + ' ' + df['Normal Flow'] + ' ' + df['Exception Flow'] + ' ' + df['Alternatif Flow']\n",
    "\n",
    "# Membagi dataset menjadi data latih dan data uji\n",
    "train, test = train_test_split(df, test_size=0.4)\n",
    "\n",
    "# Memilih kolom-kolom yang akan digunakan sebagai fitur dan label\n",
    "train_X = train[['Combined_Text']]\n",
    "train_numeric = scaler.fit_transform(train[['Jumlah Scenario', 'Jumlah kata']])\n",
    "train_y = train['Score']\n",
    "test_X = test[['Combined_Text']]\n",
    "test_y = test['Score']\n",
    "\n",
    "# Feature extraction (TF-IDF) for each column on training set\n",
    "tfidf_vectorizer_user_story = TfidfVectorizer(max_features=5000)\n",
    "X_train = tfidf_vectorizer_user_story.fit_transform(train_X['Combined_Text'])\n",
    "\n",
    "# Combine training set matrices\n",
    "X_train_combined = hstack([X_train, train_numeric])\n",
    "\n",
    "# Feature extraction (TF-IDF) for each column on test set\n",
    "X_test = tfidf_vectorizer_user_story.transform(test_X['Combined_Text'])\n",
    "\n",
    "# Combine test set matrices\n",
    "X_test_combined = hstack([X_test, scaler.transform(test[['Jumlah Scenario', 'Jumlah kata']])])\n",
    "\n",
    "# Model menggunakan LinearSVC\n",
    "svm_model = LinearSVC()  \n",
    "svm_model.fit(X_train_combined, train_y)\n",
    "y_pred_svm = svm_model.predict(X_test_combined)\n",
    "\n",
    "# Confusion Matrix for SVM\n",
    "conf_matrix_svm = confusion_matrix(test_y, y_pred_svm)\n",
    "\n",
    "# Plotting the heatmap for SVM\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix_svm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Class 1\", \"Class 2\", \"Class 3\"], yticklabels=[\"Class 1\", \"Class 2\", \"Class 3\"])\n",
    "plt.title('Confusion Matrix for Linear SVM')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.show()\n",
    "\n",
    "# Model menggunakan Adaboost dengan pengaturan algorithm='SAMME'\n",
    "adaboost_model = AdaBoostClassifier(base_estimator=LinearSVC(), n_estimators=50, algorithm='SAMME', random_state=42)\n",
    "adaboost_model.fit(X_train_combined, train_y)\n",
    "y_pred_adaboost = adaboost_model.predict(X_test_combined)\n",
    "\n",
    "# Confusion Matrix for Adaboost\n",
    "conf_matrix_adaboost = confusion_matrix(test_y, y_pred_adaboost)\n",
    "\n",
    "# Plotting the heatmap for Adaboost\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix_adaboost, annot=True, fmt=\"d\", cmap=\"Greens\", xticklabels=[\"Class 1\", \"Class 2\", \"Class 3\"], yticklabels=[\"Class 1\", \"Class 2\", \"Class 3\"])\n",
    "plt.title('Confusion Matrix for Adaboost with LinearSVC base learner (algorithm= SAMME)')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Define your models\n",
    "svm_model = LinearSVC()\n",
    "adaboost_model = AdaBoostClassifier(base_estimator=LinearSVC(), n_estimators=50, algorithm='SAMME', random_state=42)\n",
    "\n",
    "# Combine features and numeric data\n",
    "X_combined = hstack([tfidf_vectorizer_user_story.transform(df['Combined_Text']), scaler.transform(df[['Jumlah Scenario', 'Jumlah kata']])])\n",
    "\n",
    "# Perform cross-validation for Linear SVM\n",
    "svm_cv_scores = cross_val_score(svm_model, X_combined, df['Score'], cv=5)\n",
    "print(\"Cross-Validation Scores for Linear SVM:\", svm_cv_scores)\n",
    "print(\"Mean Accuracy: {:.2f}%\".format(svm_cv_scores.mean() * 100))\n",
    "\n",
    "# Perform cross-validation for AdaBoost with LinearSVC base learner\n",
    "adaboost_cv_scores = cross_val_score(adaboost_model, X_combined, df['Score'], cv=5)\n",
    "print(\"\\nCross-Validation Scores for AdaBoost with LinearSVC base learner:\", adaboost_cv_scores)\n",
    "print(\"Mean Accuracy: {:.2f}%\".format(adaboost_cv_scores.mean() * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "# Inisialisasi model Multinomial Naive Bayes\n",
    "nb_model = MultinomialNB()\n",
    "# Melatih model pada data latih\n",
    "nb_model.fit(X_train_combined, train_y)\n",
    "\n",
    "# Membuat prediksi pada data uji\n",
    "y_pred_nb = nb_model.predict(X_test_combined)\n",
    "\n",
    "# Evaluasi model Naive Bayes pada data uji\n",
    "accuracy_nb = accuracy_score(test_y, y_pred_nb)\n",
    "conf_matrix_nb = confusion_matrix(test_y, y_pred_nb)\n",
    "\n",
    "# Menampilkan hasil evaluasi\n",
    "print(\"Accuracy of Naive Bayes: {:.2f}%\".format(accuracy_nb * 100))\n",
    "print(\"\\nConfusion Matrix for Naive Bayes:\\n\", conf_matrix_nb)\n",
    "print(\"\\nClassification Report for Naive Bayes:\\n\", classification_report(test_y, y_pred_nb))\n",
    "\n",
    "# Plotting Confusion Matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix_nb, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Class 1\", \"Class 2\", \"Class 3\"], yticklabels=[\"Class 1\", \"Class 2\", \"Class 3\"])\n",
    "plt.title('Confusion Matrix for Multinomial Naive Bayes')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.show()\n",
    "\n",
    "# Inisialisasi model AdaBoost dengan Naive Bayes sebagai base estimator\n",
    "adaboost_model = AdaBoostClassifier(base_estimator=nb_model, n_estimators=50, random_state=42)\n",
    "\n",
    "# Melatih model pada data latih\n",
    "adaboost_model.fit(X_train_combined, train_y)\n",
    "\n",
    "# Membuat prediksi pada data uji\n",
    "y_pred_adaboost = adaboost_model.predict(X_test_combined)\n",
    "\n",
    "# Evaluasi model AdaBoost dengan Naive Bayes pada data uji\n",
    "accuracy_adaboost = accuracy_score(test_y, y_pred_adaboost)\n",
    "conf_matrix_adaboost = confusion_matrix(test_y, y_pred_adaboost)\n",
    "\n",
    "# Menampilkan hasil evaluasi\n",
    "print(\"Accuracy of AdaBoost with Naive Bayes: {:.2f}%\".format(accuracy_adaboost * 100))\n",
    "print(\"\\nConfusion Matrix for AdaBoost with Naive Bayes:\\n\", conf_matrix_adaboost)\n",
    "print(\"\\nClassification Report for AdaBoost with Naive Bayes:\\n\", classification_report(test_y, y_pred_adaboost))\n",
    "\n",
    "# Plotting Confusion Matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix_adaboost, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Class 1\", \"Class 2\", \"Class 3\"], yticklabels=[\"Class 1\", \"Class 2\", \"Class 3\"])\n",
    "plt.title('Confusion Matrix for AdaBoost with Multinomial Naive Bayes')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Inisialisasi model Multinomial Naive Bayes\n",
    "nb_model = MultinomialNB()\n",
    "\n",
    "# Melatih model pada data latih\n",
    "nb_model.fit(X_train_combined, train_y)\n",
    "\n",
    "# Membuat prediksi pada data uji\n",
    "y_pred_nb = nb_model.predict(X_test_combined)\n",
    "\n",
    "# Evaluasi model Naive Bayes pada data uji\n",
    "accuracy_nb = accuracy_score(test_y, y_pred_nb)\n",
    "conf_matrix_nb = confusion_matrix(test_y, y_pred_nb)\n",
    "\n",
    "# Menampilkan hasil evaluasi\n",
    "print(\"Accuracy of Naive Bayes: {:.2f}%\".format(accuracy_nb * 100))\n",
    "print(\"\\nConfusion Matrix for Naive Bayes:\\n\", conf_matrix_nb)\n",
    "print(\"\\nClassification Report for Naive Bayes:\\n\", classification_report(test_y, y_pred_nb))\n",
    "\n",
    "# Plotting Confusion Matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix_nb, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Class 1\", \"Class 2\", \"Class 3\"], yticklabels=[\"Class 1\", \"Class 2\", \"Class 3\"])\n",
    "plt.title('Confusion Matrix for Multinomial Naive Bayes')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.show()\n",
    "\n",
    "# Inisialisasi model AdaBoost dengan Naive Bayes sebagai base estimator\n",
    "adaboost_model = AdaBoostClassifier(base_estimator=nb_model, n_estimators=50, random_state=42)\n",
    "\n",
    "# Perform cross-validation for AdaBoost with Naive Bayes\n",
    "adaboost_cv_scores = cross_val_score(adaboost_model, X_train_combined, train_y, cv=5)\n",
    "print(\"\\nCross-Validation Scores for AdaBoost with Naive Bayes:\", adaboost_cv_scores)\n",
    "print(\"Mean Accuracy: {:.2f}%\".format(adaboost_cv_scores.mean() * 100))\n",
    "\n",
    "# Melatih model pada data latih (tanpa cross-validation)\n",
    "adaboost_model.fit(X_train_combined, train_y)\n",
    "\n",
    "# Membuat prediksi pada data uji\n",
    "y_pred_adaboost = adaboost_model.predict(X_test_combined)\n",
    "\n",
    "# Evaluasi model AdaBoost dengan Naive Bayes pada data uji\n",
    "accuracy_adaboost = accuracy_score(test_y, y_pred_adaboost)\n",
    "conf_matrix_adaboost = confusion_matrix(test_y, y_pred_adaboost)\n",
    "\n",
    "# Menampilkan hasil evaluasi\n",
    "print(\"\\nAccuracy of AdaBoost with Naive Bayes: {:.2f}%\".format(accuracy_adaboost * 100))\n",
    "print(\"\\nConfusion Matrix for AdaBoost with Naive Bayes:\\n\", conf_matrix_adaboost)\n",
    "print(\"\\nClassification Report for AdaBoost with Naive Bayes:\\n\", classification_report(test_y, y_pred_adaboost))\n",
    "\n",
    "# Plotting Confusion Matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix_adaboost, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Class 1\", \"Class 2\", \"Class 3\"], yticklabels=[\"Class 1\", \"Class 2\", \"Class 3\"])\n",
    "plt.title('Confusion Matrix for AdaBoost with Multinomial Naive Bayes')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coba tanpa konversi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from scipy.sparse import hstack\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Menggunakan MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "df['Combined_Text'] = df['Feature'] + ' ' + df['User Story'] + ' ' + df['Normal Flow'] + ' ' + df['Exception Flow'] + ' ' + df['Alternatif Flow']\n",
    "\n",
    "# Membagi dataset menjadi data latih dan data uji\n",
    "train, test = train_test_split(df, test_size=0.3)\n",
    "\n",
    "# Memilih kolom-kolom yang akan digunakan sebagai fitur dan label\n",
    "train_X = train[['Combined_Text']]\n",
    "train_numeric = scaler.fit_transform(train[['Jumlah Scenario', 'Jumlah kata']])\n",
    "train_y = train['Score']\n",
    "test_X = test[['Combined_Text']]\n",
    "test_y = test['Score']\n",
    "\n",
    "# Feature extraction (TF-IDF) for each column on training set\n",
    "tfidf_vectorizer_user_story = TfidfVectorizer(max_features=5000)\n",
    "X_train = tfidf_vectorizer_user_story.fit_transform(train_X['Combined_Text'])\n",
    "\n",
    "# Combine training set matrices\n",
    "X_train_combined = hstack([X_train, train_numeric])\n",
    "\n",
    "# Feature extraction (TF-IDF) for each column on test set\n",
    "X_test = tfidf_vectorizer_user_story.transform(test_X['Combined_Text'])\n",
    "\n",
    "# Combine test set matrices\n",
    "X_test_combined = hstack([X_test, scaler.transform(test[['Jumlah Scenario', 'Jumlah kata']])])\n",
    "\n",
    "# Model menggunakan LinearSVC\n",
    "svm_model = LinearSVC()  \n",
    "svm_model.fit(X_train_combined, train_y)\n",
    "\n",
    "# Evaluate svm_model on the test set\n",
    "y_pred_svm = svm_model.predict(X_test_combined)\n",
    "\n",
    "# Confusion Matrix for SVM\n",
    "conf_matrix_svm = confusion_matrix(test_y, y_pred_svm)\n",
    "\n",
    "# Plotting the heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix_svm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Class 1\", \"Class 2\", \"Class 3\"], yticklabels=[\"Class 1\", \"Class 2\", \"Class 3\"])\n",
    "plt.title('Confusion Matrix for Linear SVM')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Inisialisasi model Multinomial Naive Bayes\n",
    "nb_model = MultinomialNB()\n",
    "\n",
    "# Melatih model pada data latih\n",
    "nb_model.fit(X_train_combined, train_y)\n",
    "\n",
    "# Membuat prediksi pada data uji\n",
    "y_pred_nb = nb_model.predict(X_test_combined)\n",
    "\n",
    "# Evaluasi model Naive Bayes pada data uji\n",
    "accuracy_nb = accuracy_score(test_y, y_pred_nb)\n",
    "conf_matrix_nb = confusion_matrix(test_y, y_pred_nb)\n",
    "\n",
    "# Menampilkan hasil evaluasi\n",
    "print(\"Accuracy of Naive Bayes: {:.2f}%\".format(accuracy_nb * 100))\n",
    "print(\"\\nConfusion Matrix for Naive Bayes:\\n\", conf_matrix_nb)\n",
    "print(\"\\nClassification Report for Naive Bayes:\\n\", classification_report(test_y, y_pred_nb))\n",
    "\n",
    "# Plotting Confusion Matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix_nb, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Class 1\", \"Class 2\", \"Class 3\"], yticklabels=[\"Class 1\", \"Class 2\", \"Class 3\"])\n",
    "plt.title('Confusion Matrix for Multinomial Naive Bayes')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a= pd.DataFrame(X_train.todense().T,\n",
    "                index=tfidf_vectorizer_user_story.get_feature_names_out(),\n",
    "                columns=[f'D{i+1}' for i in range (len(train_X))])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_numeric.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a= pd.DataFrame(train_numeric_sparse.todense().T,\n",
    "                columns=[f'D{i+1}' for i in range (len(train_numeric))])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a= pd.DataFrame(X_train_combined.todense().T,\n",
    "                columns=[f'D{i+1}' for i in range (len(train_numeric))])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "from scipy.sparse import hstack\n",
    "import scipy\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "df['Combined_Text'] = df['Feature'] + ' ' + df['User Story'] + ' ' + df['Normal Flow'] + ' ' + df['Exception Flow'] + ' ' + df['Alternatif Flow']\n",
    "\n",
    "# Membagi dataset menjadi data latih dan data uji\n",
    "train, test = train_test_split(df, test_size=0.3)\n",
    "\n",
    "# Memilih kolom-kolom yang akan digunakan sebagai fitur dan label\n",
    "train_X = train[['Combined_Text']]\n",
    "train_numeric = scaler.fit_transform(train[['Jumlah Scenario']])\n",
    "train_y = train['Score']\n",
    "test_X = test[['Combined_Text']]\n",
    "test_y = test['Score']\n",
    "\n",
    "# Feature extraction (TF-IDF) for each column on training set\n",
    "tfidf_vectorizer_user_story = TfidfVectorizer(max_features=5000)\n",
    "X_train = tfidf_vectorizer_user_story.fit_transform(train_X['Combined_Text'])\n",
    "\n",
    "# Convert matriks numerik train set menjadi matriks sparse\n",
    "train_numeric_sparse = scipy.sparse.csr_matrix(train_numeric)\n",
    "\n",
    "# Combine training set matrices\n",
    "X_train_combined = hstack([X_train, train_numeric_sparse])\n",
    "\n",
    "# Feature extraction (TF-IDF) for each column on test set\n",
    "X_test = tfidf_vectorizer_user_story.transform(test_X['Combined_Text'])\n",
    "\n",
    "# Convert matriks numerik test set menjadi matriks sparse\n",
    "test_numeric_sparse = scipy.sparse.csr_matrix(scaler.transform(test[['Jumlah Scenario']]))\n",
    "\n",
    "# Combine test set matrices\n",
    "X_test_combined = hstack([X_test, test_numeric_sparse])\n",
    "\n",
    "\n",
    "svm_model = LinearSVC()  \n",
    "svm_model.fit(X_train_combined, train_y)\n",
    "\n",
    "# Evaluate svm_model on the test set\n",
    "y_pred_svm = svm_model.predict(X_test_combined)\n",
    "\n",
    "# Confusion Matrix for SVM\n",
    "conf_matrix_svm = confusion_matrix(test_y, y_pred_svm)\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Plotting the heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix_svm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Class 1\", \"Class 2\", \"Class 3\"], yticklabels=[\"Class 1\", \"Class 2\", \"Class 3\"])\n",
    "plt.title('Confusion Matrix for Linear SVM')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix_svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train the Logistic Regression model\n",
    "logistic_model = LogisticRegression()\n",
    "logistic_model.fit(X_train_combined, train_y)\n",
    "\n",
    "# Evaluate Logistic Regression on the test set\n",
    "y_pred_logistic = logistic_model.predict(X_test_combined)\n",
    "\n",
    "# Confusion Matrix for Logistic Regression\n",
    "conf_matrix_logistic = confusion_matrix(test_y, y_pred_logistic)\n",
    "print(\"Confusion Matrix for Logistic Regression:\")\n",
    "print(conf_matrix_logistic)\n",
    "\n",
    "# Additional evaluation metrics\n",
    "accuracy_logistic = accuracy_score(test_y, y_pred_logistic)\n",
    "print(f\"\\nAccuracy for Logistic Regression: {accuracy_logistic:.2f}\")\n",
    "\n",
    "classification_report_logistic = classification_report(test_y, y_pred_logistic)\n",
    "print(\"\\nClassification Report for Logistic Regression:\")\n",
    "print(classification_report_logistic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "rf_model = RandomForestClassifier()\n",
    "rf_model.fit(X_train_combined, train_y)\n",
    "\n",
    "# Evaluate Random Forest on the test set\n",
    "y_pred_rf = rf_model.predict(X_test_combined)\n",
    "\n",
    "# Confusion Matrix for Random Forest\n",
    "conf_matrix_rf = confusion_matrix(test_y, y_pred_rf)\n",
    "print(\"\\nConfusion Matrix for Random Forest:\")\n",
    "print(conf_matrix_rf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stacking classifier without final_estimator\n",
    "stacking_model = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('svm', svm_model),\n",
    "        ('lr', logistic_model),\n",
    "        ('rf', rf_model)\n",
    "    ],\n",
    "    cv=2  # Adjust the number of splits as needed\n",
    ")\n",
    "\n",
    "# Train the stacking model\n",
    "stacking_model.fit(X_train_combined, train_y)\n",
    "\n",
    "# Evaluate stacking model on the test set\n",
    "y_pred_stacking = stacking_model.predict(X_test_combined)\n",
    "\n",
    "# Confusion Matrix for Stacking\n",
    "conf_matrix_stacking = confusion_matrix(test_y, y_pred_stacking)\n",
    "print(\"Confusion Matrix for Stacking:\")\n",
    "print(conf_matrix_stacking)\n",
    "\n",
    "# Additional evaluation metrics\n",
    "accuracy_stacking = accuracy_score(test_y, y_pred_stacking)\n",
    "print(f\"\\nAccuracy for Stacking: {accuracy_stacking:.2f}\")\n",
    "\n",
    "classification_report_stacking = classification_report(test_y, y_pred_stacking)\n",
    "print(\"\\nClassification Report for Stacking:\")\n",
    "print(classification_report_stacking)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Menampilkan matriks sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a= pd.DataFrame(X_train.todense().T,\n",
    "                index=tfidf_vectorizer_user_story.get_feature_names_out(),\n",
    "                columns=[f'D{i+1}' for i in range (len(train_X))])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a= pd.DataFrame(train_numeric_sparse.todense().T,\n",
    "                columns=[f'D{i+1}' for i in range (len(train_numeric))])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a= pd.DataFrame(X_train_combined.todense().T,\n",
    "                columns=[f'D{i+1}' for i in range (len(train_numeric))])\n",
    "a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
